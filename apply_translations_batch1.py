#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
批量翻译脚本 - 分批次处理
"""

import json
import sys

# 第一批翻译(1-10篇)
translations_batch1 = {
    1: "近年来大语言模型(LLM)的进步使自主智能体能够执行需要与工具和环境进行多轮交互的复杂任务。然而,这类智能体训练的规模化受到缺乏多样化和可靠环境的限制。在本文中,我们提出了智能体世界模型(AWM),一个完全合成的环境生成流水线。使用此流水线,我们扩展到1000个涵盖日常场景的环境,智能体可以在其中与丰富的工具集(平均每个环境35个工具)交互并获得高质量的观察。值得注意的是,这些环境由代码驱动并由数据库支持,相比LLM模拟的环境提供更可靠和一致的状态转换。此外,与从真实环境中收集轨迹相比,它们实现了更高效的智能体交互。为了展示此资源的有效性,我们对多轮工具使用智能体进行了大规模强化学习。得益于完全可执行的环境和可访问的数据库状态,我们还可以设计可靠的奖励函数。在三个基准测试上的实验表明,在合成环境而非特定基准环境中独家训练,可以产生强大的分布外泛化能力。代码可在https://github.com/Snowflake-Labs/agent-world-model获取。",
    
    2: "记忆对于在单个上下文窗口之外运行的大语言模型(LLM)智能体越来越重要,但大多数现有系统依赖离线、与查询无关的记忆构建,这可能效率低下并可能丢弃查询关键信息。尽管运行时记忆利用是一个自然的替代方案,但先前的工作通常会产生大量开销,并且对性能-成本权衡的显式控制有限。在这项工作中,我们提出了BudgetMem,一个用于显式、查询感知的性能-成本控制的运行时智能体记忆框架。BudgetMem将记忆处理结构化为一组记忆模块,每个模块提供三个预算层级(即低/中/高)。一个轻量级路由器执行跨模块的预算层级路由,以平衡任务性能和记忆构建成本,这是通过强化学习训练的紧凑神经策略实现的。使用BudgetMem作为统一测试平台,我们研究了三种互补策略来实现预算层级:实现(方法复杂性)、推理(推理行为)和容量(模块模型大小)。在LoCoMo、LongMemEval和HotpotQA上,BudgetMem在优先考虑性能时(即高预算设置)超越了强基线,并在更紧预算下提供更好的准确性-成本前沿。此外,我们的分析解开了不同分层策略的优势和劣势,阐明了在不同预算制度下每个轴何时提供最有利的权衡。",
    
    3: "块稀疏注意力对于加速长上下文LLM预填充很有前景,但有效识别相关块仍然是一个瓶颈。现有方法通常采用粗粒度注意力作为块重要性估计的代理,但经常求助于昂贵的token级搜索或评分,导致显著的选择开销。在这项工作中,我们追溯了通过均值池化的标准粗粒度注意力的不准确性到理论根本原因:均值池化和旋转位置嵌入(RoPE)之间的相互作用。我们证明均值池化充当低通滤波器,在高频维度中引起破坏性干扰,有效地为局部位置信息(例如斜杠模式)创建\"盲点\"。为了解决这个问题,我们引入了Prism,一种无需训练的频谱感知方法,将块选择分解为高频和低频分支。通过应用基于能量的温度校准,Prism直接从池化表示中恢复衰减的位置信号,使用纯块级操作实现块重要性估计,从而提高效率。广泛的评估证实,Prism在保持与完全注意力准确性一致的同时,提供高达5.1倍的加速。",
    
    4: "能够在各种环境中模拟动作结果将彻底改变大规模通用智能体的开发。然而,建模这些世界动态,特别是对于灵巧的机器人任务,由于数据覆盖有限和动作标签稀缺而面临重大挑战。作为实现这一目标的努力,我们引入了DreamDojo,一个从44000小时的自我中心人类视频中学习多样化交互和灵巧控制的基础世界模型。我们的数据混合代表迄今为止最大的世界模型预训练视频数据集,涵盖各种日常场景,包含多样化的对象和技能。为了解决动作标签的稀缺性,我们引入连续潜在动作作为统一代理动作,增强从未标记视频的交互知识转移。在小规模目标机器人数据上进行后训练后,DreamDojo展示了对物理的强大理解和精确的动作可控性。我们还设计了一个蒸馏流水线,将DreamDojo加速到10.81 FPS的实时速度,并进一步提高上下文一致性。我们的工作实现了几个基于生成式世界模型的重要应用,包括实时远程操作、策略评估和基于模型的规划。在多个具有挑战性的分布外(OOD)基准测试上的系统评估验证了我们的方法对于模拟开放世界、接触丰富任务的重要性,为通用机器人世界模型铺平了道路。",
    
    5: "解决开放式科学问题对大语言模型仍然具有挑战性,特别是由于固有的不可靠监督和评估。瓶颈在于科学后训练的数据构建和奖励设计。我们开发了一个大规模、系统的数据处理流水线,将异构开源科学数据转换为Dr. SCI数据集,该数据集包含八个STEM学科的100万个问题,具有明确的可验证/开放式拆分、可扩展的难度注释,以及将开放式答案的评估操作化的细粒度评分标准。基于此数据集,我们提出了Dr. SCI后训练流水线,通过三个组件重新设计了标准的SFT -> RL工作流:(i)探索扩展SFT,在RL之前扩展模型的推理模式覆盖范围;(ii)动态难度课程,使训练数据适应模型不断演变的科学能力;(iii)SciRubric引导的RL,通过基于评分标准的评估和明确的答案正确性,实现对开放式科学问题的稳定强化学习。使用Dr. SCI流水线训练的Qwen3-4B-Base在GPQA-diamond上达到63.2,在GPQA-general上达到32.4,持续改进优于o1-mini和GPT-4o等强大的后训练基线,在科学推理方面展示了实质性收益,特别是在开放式设置中。",
    
    6: "大语言模型(LLM)的规模化推动了对基于矩阵的优化器(如Shampoo、Muon、SOAP)的兴趣,因为它们具有收敛效率;然而,它们对整体更新的要求与Megatron等分布式框架中的张量碎片化相冲突。现有解决方案是次优的:同步方法存在计算冗余,而逐层分区在不违反高效通信原语的几何约束的情况下无法调和这种冲突。为了弥合这一差距,我们提出了Canzona,一个统一、异步和负载平衡的框架,将逻辑优化器分配与物理参数分布解耦。对于数据并行,我们引入了一种α平衡静态分区策略,在尊重原子性的同时中和负载不平衡。对于张量并行,我们设计了一个异步计算流水线,利用微组调度来批处理碎片更新并隐藏重建开销。在256个GPU上对Qwen3模型系列(高达32B参数)的广泛评估表明,我们的方法保留了已建立的并行架构的效率,与基线相比,端到端迭代时间加速1.57倍,优化器步骤延迟减少5.8倍。",
    
    7: "具有可验证奖励的强化学习(RLVR)已成为增强大语言模型(LLM)推理的不可或缺的范式。然而,标准策略优化方法,如群组相对策略优化(GRPO),经常收敛到低熵策略,导致严重的模式崩溃和有限的输出多样性。我们从采样概率动态的角度分析这个问题,识别出标准目标不成比例地强化最高可能性路径,从而抑制有效的替代推理链。为了解决这个问题,我们提出了一种新颖的优势重加权机制(ARM),旨在平衡所有正确响应的置信水平。通过将提示困惑度和答案置信度纳入优势估计,我们的方法动态重塑奖励信号,以衰减过度自信推理路径的梯度更新,同时将概率质量重新分配给探索不足的正确解决方案。实证结果表明,我们的方法在保持竞争准确性的同时显著增强了生成多样性和响应熵,有效地在推理任务中实现了探索和利用之间的卓越权衡。在Qwen2.5和DeepSeek模型上跨数学和编码基准的实证结果表明,ProGRPO显著缓解了熵崩溃。具体而言,在Qwen2.5-7B上,我们的方法在Pass@1上比GRPO提高了5.7%,值得注意的是,在Pass@32上提高了13.9%,突出了其在生成多样化正确推理路径方面的卓越能力。",
    
    8: "大型推理模型(LRM)通过生成长的、多步骤的推理轨迹在复杂推理任务上取得了强大的性能,但推理时扩展会产生大量部署成本。一个关键挑战是生成难度在单个输出内变化,而现有的面向效率的方法要么忽略这种生成内变化,要么依赖于具有高系统复杂性的监督token级路由。我们提出了RelayGen,一个无需训练的、段级运行时模型切换框架,利用长形式推理中的难度变化。通过使用token概率边际对生成不确定性进行离线分析,我们表明粗粒度段级控制足以捕获推理轨迹内的难度转换。RelayGen识别特定于模型的切换提示,这些提示信号转换到较低难度段,并动态将其继续委托给较小的模型,同时在大模型上保留高难度推理。在多个推理基准测试中,RelayGen在保留大模型大部分准确性的同时大幅减少了推理延迟。当与推测解码结合使用时,RelayGen实现了高达2.2倍的端到端加速,准确性降低不到2%,无需额外训练或学习路由组件。",
    
    9: "当前的移动GUI智能体基准系统性地无法评估记忆能力,只有5.2-11.8%的记忆相关任务,并且没有跨会话学习评估。我们引入了MemGUI-Bench,一个全面的以记忆为中心的基准,具有pass@k和分阶段的LLM作为裁判评估。我们的贡献包括:(1)一个系统的记忆分类法,分析5种架构的11个智能体;(2)26个应用程序中的128个任务,其中89.8%通过跨时间和跨空间保留来挑战记忆;(3)MemGUI-Eval,一个具有渐进审查和7个分层指标的自动化流水线;(4)对11个最先进智能体的RQ驱动评估。我们的实验揭示了所有评估系统中的显著记忆缺陷,识别了5种不同的失败模式,并综合了5个可操作的设计含义。所有资源包括代码、基准和评估结果将在https://lgy0404.github.io/MemGUI-Bench/完全开源并持续维护。",
    
    10: "演绎、归纳和溯因是基本的推理范式,是人类逻辑思维的核心。尽管改进大语言模型(LLM)推理已经吸引了大量研究努力,但基本范式诱导泛化的程度尚未得到系统探索。在这项研究中,我们阐明了这些核心范式之间的相互作用如何影响LLM的推理行为。为此,我们首先从符号任务中收集了一个新的推理轨迹数据集,每个任务针对三个基本范式之一,以从具体的世界知识中抽象出来。然后,我们研究将这些技能诱导到LLM中的有效方法。我们使用一系列方法进行实验,包括简单的微调,以及更复杂的方法来增加模型深度,或将密集模型转换为混合专家。我们在完全用自然语言表述并包含真实世界知识的现实分布外任务上全面评估诱导模型。我们的结果表明,我们的方法在现实任务中产生了强大的泛化能力,性能提升显著(高达14.60)。"
}

def apply_translations(input_file, output_file, translations):
    """应用翻译到JSON文件"""
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    for item in data:
        if item['id'] in translations:
            item['chinese'] = translations[item['id']]
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    translated_count = len([item for item in data if item['chinese']])
    print(f"✅ 已翻译 {translated_count}/{len(data)} 篇摘要")
    return translated_count

if __name__ == "__main__":
    input_file = "/data/workspace/papers-weekly-site/abstracts.json"
    
    # 应用第一批翻译
    count = apply_translations(input_file, input_file, translations_batch1)
    print(f"📝 进度: {count}/80 ({count/80*100:.1f}%)")
