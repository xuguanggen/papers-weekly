#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import json

# 第2-3批翻译(11-30篇)
translations_batch2_3 = {
    11: "金融市场嘈杂且非平稳,使得阿尔法挖掘对回测结果中的噪声和突然的市场制度转变高度敏感。虽然最近的智能体框架改进了阿尔法挖掘自动化,但它们通常缺乏可控的多轮搜索和可靠的经验重用。为了应对这些挑战,我们提出了QuantaAlpha,一个进化的阿尔法挖掘框架,将每次端到端挖掘运行视为一个轨迹,并通过轨迹级突变和交叉操作改进因子。QuantaAlpha定位每个轨迹中的次优步骤进行针对性修订,并重新组合互补的高奖励片段以重用有效模式,实现跨挖掘迭代的结构化探索和细化。在因子生成过程中,QuantaAlpha在假设、因子表达和可执行代码之间强制语义一致性,同时约束生成因子的复杂性和冗余以缓解拥挤。在中证300指数(CSI 300)上的广泛实验展示了相对于强基线模型和先前智能体系统的一致收益。当使用GPT-5.2时,QuantaAlpha实现0.1501的信息系数(IC)、27.75%的年化收益率(ARR)和7.98%的最大回撤(MDD)。此外,在CSI 300上挖掘的因子有效转移到中证500指数(CSI 500)和标准普尔500指数(S&P 500),在四年内分别提供160%和137%的累计超额回报,这表明QuantaAlpha在市场分布转变下具有强大的鲁棒性。",
    
    12: "尽管多模态对比学习在对齐视觉和语言表示方面取得了成功,但一个持续存在的几何异常,即模态差距仍然存在:表达相同语义的不同模态的嵌入占据系统偏移的区域。弥合这一差距的先前方法在很大程度上受到过度简化的各向同性假设的限制,阻碍了它们在大规模场景中的应用。在本文中,我们通过精确表征模态差距的几何形状并利用它进行高效的模型扩展来解决这些限制。首先,我们提出固定框架模态差距理论,该理论在冻结的参考框架内将模态差距分解为稳定偏差和各向异性残差。在这种精确建模的指导下,我们引入了ReAlign,一种无需训练的模态对齐策略。利用来自大量未配对数据的统计信息,ReAlign通过包括锚点、轨迹和质心对齐的三步过程将文本表示对齐到图像表示分布中,从而明确纠正几何错位。基于ReAlign,我们提出了ReVision,一个用于多模态大语言模型(MLLM)的可扩展训练范式。ReVision将ReAlign集成到预训练阶段,使模型能够在视觉指令调优之前从未配对文本中学习视觉表示的分布,而无需大规模、高质量的图像-文本对。我们的框架展示了统计对齐的未配对数据可以有效替代昂贵的图像-文本对,为MLLM的高效扩展提供了强大的路径。",
    
    13: "音频对于真实世界的视频是不可或缺的,但生成模型在很大程度上忽视了音频组件。当前生成视听内容的方法通常依赖级联流水线,这增加了成本、累积错误并降低了整体质量。虽然Veo 3和Sora 2等系统强调同时生成的价值,但联合多模态建模在架构、数据和训练方面带来了独特的挑战。此外,现有系统的闭源性质限制了该领域的进展。在这项工作中,我们引入了MOVA(MOSS视频和音频),一个能够生成高质量、同步视听内容的开源模型,包括逼真的唇音同步语音、环境感知音效和内容对齐音乐。MOVA采用混合专家(MoE)架构,总共有32B参数,其中18B在推理过程中处于活动状态。它支持IT2VA(图像-文本到视频-音频)生成任务。通过发布模型权重和代码,我们旨在推进研究并培养一个充满活力的创作者社区。发布的代码库为高效推理、LoRA微调和提示增强提供全面支持。",
    
    14: "随着高质量公共文本接近枯竭,这一现象被称为数据墙,预训练正在从更多token转向更好的token。然而,现有方法要么依赖忽略训练动态的启发式静态过滤器,要么使用基于原始梯度的动态但与优化器无关的标准。我们提出OPUS(优化器诱导的投影效用选择),一个在优化器诱导的更新空间中定义效用的动态数据选择框架。OPUS通过将由现代优化器塑造的有效更新投影到从稳定的分布内代理派生的目标方向上来对候选者进行评分。为确保可扩展性,我们使用CountSketch的Ghost技术实现计算效率,并使用Boltzmann采样实现数据多样性,仅产生4.7%的额外计算开销。OPUS在各种语料库、质量层级、优化器和模型规模上取得了显著成果。在FineWeb和FineWeb-Edu上使用30B token对GPT-2 Large/XL进行预训练时,OPUS优于工业级基线,甚至超过完整的200B token训练。此外,当与工业级静态过滤器结合使用时,OPUS进一步提高了预训练效率,即使使用较低质量的数据。此外,在SciencePedia上对Qwen3-8B-Base进行持续预训练时,OPUS仅使用0.5B token就实现了优于使用3B token的完整训练的卓越性能,展示了在专业领域的显著数据效率收益。",
    
    15: "随着后训练优化成为改进大语言模型的核心,我们观察到一个持续存在的饱和瓶颈:一旦模型变得高度自信,进一步的训练产生递减的回报。虽然现有方法继续强化目标预测,但我们发现信息性监督信号仍然潜伏在模型自身的历史弱状态中。受此观察的启发,我们提出了WMSS(弱智能体可以使强智能体更强),一个利用弱检查点指导持续优化的后训练范式。通过熵动态识别可恢复的学习差距并通过补偿学习强化它们,WMSS使强智能体能够超越传统的后训练饱和。在数学推理和代码生成数据集上的实验表明,使用我们方法训练的智能体实现了有效的性能改进,同时产生零额外推理成本。",
    
    16: "自主GUI智能体通过感知界面和执行动作与环境交互。作为虚拟沙箱,GUI世界模型通过启用动作条件预测为智能体赋予类似人类的远见。然而,现有的基于文本和像素的方法难以同时实现高视觉保真度和细粒度结构可控性。为此,我们提出Code2World,一个通过可渲染代码生成模拟下一个视觉状态的视觉-语言编码器。具体来说,为了解决数据稀缺问题,我们通过将GUI轨迹转换为高保真HTML并通过视觉反馈修订机制完善合成代码来构建AndroidCode,产生超过80K个高质量屏幕-动作对的语料库。为了使现有VLM适应代码预测,我们首先执行SFT作为格式布局遵循的冷启动,然后进一步应用渲染感知强化学习,该学习使用渲染结果作为奖励信号,通过强制视觉语义保真度和动作一致性。广泛的实验表明,Code2World-8B实现了顶级的下一个UI预测,与竞争激烈的GPT-5和Gemini-3-Pro-Image相媲美。值得注意的是,Code2World以灵活的方式显著提高了下游导航成功率,在AndroidWorld导航上将Gemini-2.5-Flash提高了+9.5%。代码可在https://github.com/AMAP-ML/Code2World获取。",
    
    17: "具有可验证奖励的强化学习(RLVR)通常基于群组采样来估计优势并稳定策略更新。实际上,由于计算限制,大群组规模是不可行的,这使学习偏向于已经可能的轨迹。较小的群组经常错过罕见正确的轨迹,同时仍然包含混合奖励,将概率集中在常见解决方案上。我们推导出更新错过罕见正确模式的概率作为群组大小的函数,显示非单调行为,并表征更新如何在正确集内重新分配质量,揭示未采样正确质量可以收缩,即使总正确质量增长。受此分析的启发,我们提出了一个受Focal损失启发的难度感知优势缩放系数,该系数降低高成功提示的更新权重。轻量级修改可以直接集成到任何群组相对RLVR算法中,如GRPO、DAPO和CISPO。在Qwen2.5-7B上跨领域内和领域外基准测试,我们的方法将pass@256从64.1提高到70.3(GRPO)、69.3提高到72.5(DAPO)和73.2提高到76.8(CISPO),同时保留或提高pass@1,而不增加群组大小或计算成本。",
    
    18: "LLM智能体在推进科学研究方面具有重大前景。为了加速这一进展,我们引入了AIRS-Bench(AI研究科学基准),这是一套来自最先进机器学习论文的20个任务。这些任务跨越多个领域,包括语言建模、数学、生物信息学和时间序列预测。AIRS-Bench任务评估整个研究生命周期的智能体能力-包括想法生成、实验分析和迭代细化-而不提供基线代码。AIRS-Bench任务格式是通用的,可以轻松集成新任务并在不同智能体框架之间进行严格比较。我们使用配备序列和并行脚手架的前沿模型建立基线。我们的结果表明,智能体在四个任务中超过了人类SOTA,但在其他十六个任务中未能匹配。即使智能体超越人类基准,它们也没有达到基础任务的理论性能上限。这些发现表明AIRS-Bench远未饱和,并提供了大量改进空间。我们开源了AIRS-Bench任务定义和评估代码,以促进自主科学研究的进一步发展。",
    
    19: "当前的视觉-语言-动作(VLA)模型依赖于固定的计算深度,在简单调整和复杂的多步骤操纵上花费相同数量的计算。虽然思维链(CoT)提示实现了可变计算,但它线性扩展内存,不适合连续动作空间。我们引入递归深度VLA(RD-VLA),一种通过潜在迭代细化而非显式token生成实现计算自适应性的架构。RD-VLA采用递归、权重绑定的动作头,支持具有恒定内存占用的任意推理深度。该模型使用截断的时间反向传播(TBPTT)进行训练,以有效监督细化过程。在推理时,RD-VLA使用基于潜在收敛的自适应停止标准动态分配计算。在具有挑战性的操纵任务上的实验表明,递归深度至关重要:使用单次迭代推理完全失败(0%成功)的任务在四次迭代后超过90%成功,而更简单的任务迅速饱和。RD-VLA为机器人技术中的测试时计算提供了一条可扩展的路径,用潜在推理替代基于token的推理,实现恒定内存使用,并比先前基于推理的VLA模型快80倍。项目页面:https://rd-vla.github.io/",
    
    20: "GUI智能体已成为自动化数字环境交互的强大范式,但实现广泛通用性和持续强大的任务性能仍然具有挑战性。在本报告中,我们提出UI-Venus-1.5,一个统一的端到端GUI智能体,专为强大的真实世界应用而设计。所提出的模型系列包括两个密集变体(2B和8B)和一个混合专家变体(30B-A3B),以满足各种下游应用场景。与我们之前的版本相比,UI-Venus-1.5引入了三个关键技术进步:(1)利用30+数据集的100亿token的全面中期训练阶段,以建立基础GUI语义;(2)具有完整轨迹展开的在线强化学习,将训练目标与大规模环境中的长期动态导航对齐;(3)通过模型合并构建的单一统一GUI智能体,将特定领域模型(接地、网络和移动)合成为一个内聚检查点。广泛的评估表明,UI-Venus-1.5在ScreenSpot-Pro(69.6%)、VenusBench-GD(75.0%)和AndroidWorld(77.6%)等基准测试上建立了新的最先进性能,显著优于先前的强基线。此外,UI-Venus-1.5展示了跨各种中文移动应用程序的强大导航能力,有效执行真实世界场景中的用户指令。代码:https://github.com/inclusionAI/UI-Venus;模型:https://huggingface.co/collections/inclusionAI/ui-venus",
    
    21: "虽然LLaDA2.0展示了100B级块扩散模型的扩展潜力及其固有的并行化,但解码速度和生成质量之间的微妙平衡一直是一个难以捉摸的前沿。今天,我们推出LLaDA2.1,一个旨在超越这种权衡的范式转变。通过将Token到Token(T2T)编辑无缝编织到传统的Mask到Token(M2T)方案中,我们引入了一个联合的、可配置的阈值解码方案。这种结构创新产生了两种不同的角色:速度模式(S模式),大胆降低M2T阈值以绕过传统约束,同时依赖T2T来细化输出;以及质量模式(Q模式),倾向于保守阈值以确保卓越的基准性能,管理效率降级。进一步推进这一演变,在广泛的上下文窗口支持下,我们实施了第一个专门为dLLM量身定制的大规模强化学习(RL)框架,由稳定梯度估计的专门技术锚定。这种对齐不仅提高了推理精度,而且提升了指令遵循保真度,弥合了扩散动态与复杂人类意图之间的鸿沟。我们通过发布LLaDA2.1-Mini(16B)和LLaDA2.1-Flash(100B)来完成这项工作。在33个严格的基准测试中,LLaDA2.1提供强大的任务性能和闪电般的解码速度。尽管其体积为100B,在编码任务上,它在HumanEval+上达到惊人的892 TPS,在BigCodeBench上达到801 TPS,在LiveCodeBench上达到663 TPS。",
    
    22: "稀疏自动编码器(SAE)是解释神经表示的强大工具,但它们在音频中的使用仍未得到充分探索。我们在Whisper和HuBERT的所有编码器层上训练SAE,对其稳定性、可解释性进行广泛评估,并展示其实际效用。超过50%的特征在随机种子之间保持一致,重建质量得以保留。SAE特征捕获一般声学和语义信息以及特定事件,包括环境噪声和副语言声音(例如笑声、耳语),并有效解开它们,只需要删除19-27%的特征即可擦除概念。特征操纵将Whisper的假语音检测减少70%,WER增加可忽略不计,展示了真实世界的适用性。最后,我们发现SAE特征与语音感知期间的人类EEG活动相关,表明与人类神经处理对齐。代码和检查点可在https://github.com/audiosae/audiosae_demo获取。",
    
    23: "熵是衡量大语言模型(LLM)生成的输出多样性的关键指标,为其探索能力提供了有价值的见解。虽然最近的研究越来越关注监控和调整熵,以更好地平衡强化微调(RFT)中的探索和利用,但对此过程中熵动态的原则性理解尚未得到彻底研究。在本文中,我们建立了一个理论框架来分析RFT过程中的熵动态,该框架始于一个判别表达式,该表达式量化单个logit更新下的熵变化。这个基础使得能够推导熵变化的一阶表达式,该表达式可以进一步扩展到群组相对策略优化(GRPO)的更新公式。从理论分析中得出的推论和见解启发了熵控制方法的设计,并为解释现有研究中各种基于熵的方法提供了统一的视角。我们提供经验证据来支持我们分析的主要结论,并展示了派生的熵判别器裁剪方法的有效性。这项研究为RFT训练动态提供了新颖的见解,为优化LLM微调过程中的探索-利用平衡提供了理论支持和实用策略。",
    
    24: "大语言模型(LLM)的快速发展催化了能够导航复杂环境的自主智能体的发展。然而,现有评估主要采用演绎范式,智能体根据明确提供的规则和静态目标执行任务,通常在有限的规划范围内。至关重要的是,这忽略了智能体从经验中自主发现潜在转换规律的归纳必要性,这是实现智能体远见和维持战略连贯性的基石。为了弥合这一差距,我们引入OdysseyArena,它将智能体评估重新集中在长期、主动和归纳交互上。我们形式化并实例化四个原语,将抽象转换动态转换为具体的交互环境。在此基础上,我们建立了OdysseyArena-Lite进行标准化基准测试,提供一组120个任务来衡量智能体的归纳效率和长期发现。进一步推进,我们引入OdysseyArena-Challenge来压力测试智能体在极端交互范围(例如>200步)下的稳定性。对15+领先LLM的广泛实验表明,即使是前沿模型也表现出归纳场景中的缺陷,识别出在复杂环境中追求自主发现的关键瓶颈。我们的代码和数据可在https://github.com/xufangzhi/Odyssey-Arena获取",
    
    25: "我们引入Baichuan-M3,一个医疗增强的大语言模型,旨在将范式从被动问答转变为主动的临床级决策支持。针对现有系统在开放式咨询中的局限性,Baichuan-M3利用专门的训练流水线对医生的系统工作流程进行建模。关键能力包括:(i)主动信息获取以解决歧义;(ii)将分散证据统一为连贯诊断的长期推理;(iii)自适应幻觉抑制以确保事实可靠性。实证评估表明,Baichuan-M3在HealthBench、新引入的HealthBench-Hallu和ScanBench上取得了最先进的结果,在临床查询、咨询和安全方面显著优于GPT-5.2。这些模型可在https://huggingface.co/collections/baichuan-inc/baichuan-m3公开获取。",
    
    26: "在流匹配模型上部署GRPO已被证明对文本到图像生成有效。然而,现有范式通常将基于结果的奖励传播到所有先前的去噪步骤,而不区分每个步骤的局部效果。此外,当前的群组排名主要在匹配的时间步长比较轨迹,并忽略轨迹内依赖关系,其中某些早期去噪动作可以通过延迟的隐式交互影响后续状态。我们提出TurningPoint-GRPO(TP-GRPO),一个GRPO框架,缓解步级奖励稀疏性并明确建模去噪轨迹内的长期效应。TP-GRPO做出两个关键创新:(i)它用步级增量奖励替换基于结果的奖励,提供密集的、步感知的学习信号,更好地隔离每个去噪动作的\"纯\"效果,(ii)它识别转折点-翻转局部奖励趋势并使后续奖励演变与整体轨迹趋势一致的步骤-并为这些动作分配聚合的长期奖励以捕获其延迟影响。转折点仅通过增量奖励的符号变化来检测,使TP-GRPO高效且无超参数。广泛的实验还表明,TP-GRPO更有效地利用奖励信号并持续改进生成。演示代码可在https://github.com/YunzeTong/TurningPoint-GRPO获取。",
    
    27: "大语言模型(LLM)智能体在复杂任务中表现出惊人的结果,但它们经常孤立运行,无法从过去的经验中学习。现有的基于记忆的方法主要存储原始轨迹,这些轨迹通常是冗余和噪声密集的。这阻止智能体提取对泛化至关重要的高级、可重用的行为模式。在本文中,我们提出SkillRL,一个通过自动技能发现和递归演化弥合原始经验和策略改进之间差距的框架。我们的方法引入了一个基于经验的蒸馏机制来构建分层技能库SkillBank,一个用于通用和任务特定启发式的自适应检索策略,以及一个允许技能库在强化学习过程中与智能体策略共同演化的递归演化机制。这些创新显著减少了token占用,同时增强了推理效用。在ALFWorld、WebShop和七个搜索增强任务上的实验结果表明,SkillRL实现了最先进的性能,优于强基线超过15.3%,并随着任务复杂性的增加保持鲁棒性。代码可在此https://github.com/aiming-lab/SkillRL获取。",
    
    28: "人工智能与材料科学的融合提供了一个变革性的机会,但实现发现的真正加速需要超越任务隔离的微调模型,转向在整个发现循环中规划、行动和学习的智能体系统。本综述提出了一个独特的以流水线为中心的视角,从语料库管理和预训练,通过领域适应和指令调优,到与仿真和实验平台接口的目标条件智能体。与先前的综述不同,我们将整个过程视为一个端到端系统,以针对有形的发现结果而非代理基准进行优化。这种观点使我们能够追踪上游设计选择-如数据管理和训练目标-如何通过有效的信用分配与下游实验成功对齐。为了弥合社区并建立共享的参考框架,我们首先提出一个集成视角,对齐AI和材料科学的术语、评估和工作流程阶段。然后,我们通过两个聚焦视角分析该领域:从AI角度,该综述详细介绍了LLM在模式识别、预测分析和用于文献挖掘、材料表征和性质预测的自然语言处理方面的优势;从材料科学角度,它突出了材料设计、过程优化以及通过与外部工具(例如DFT、机器人实验室)集成加速计算工作流程的应用。最后,我们将被动、反应方法与智能体设计进行对比,对当前贡献进行分类,同时激励追求具有自主性、记忆和工具使用的长期目标的系统。本综述绘制了一条通往自主、安全感知的LLM智能体的实用路线图,旨在发现新颖和有用的材料。",
    
    29: "我们引入InternAgent-1.5,一个为跨计算和经验领域的端到端科学发现而设计的统一系统。该系统建立在由三个协调子系统组成的结构化架构上,用于生成、验证和演化。这些子系统由深度研究、解决方案优化和长期记忆的基础能力支持。该架构允许InternAgent-1.5在扩展的发现周期中持续运行,同时保持连贯和改进的行为。它还使系统能够在单个统一系统内协调计算建模和实验室实验。我们在GAIA、HLE、GPQA和FrontierScience等科学推理基准测试上评估InternAgent-1.5,该系统实现了领先性能,展示了强大的基础能力。除了这些基准测试,我们进一步评估了两类发现任务。在算法发现任务中,InternAgent-1.5自主设计核心机器学习问题的竞争方法。在经验发现任务中,它执行完整的计算或湿实验室实验,并在地球、生命、生物和物理领域产生科学发现。总体而言,这些结果表明InternAgent-1.5为自主科学发现提供了一个通用且可扩展的框架。",
    
    30: "直接在物理世界中进行在线策略学习是具身智能的一个有前景但具有挑战性的方向。与仿真不同,真实世界系统不能任意加速、廉价重置或大规模复制,这使得可扩展的数据收集、异构部署和长期有效训练变得困难。这些挑战表明,真实世界策略学习不仅是一个算法问题,而且从根本上是一个系统问题。我们提出USER,一个用于真实世界在线策略学习的统一且可扩展的系统。USER通过统一的硬件抽象层将物理机器人视为与GPU并列的一流硬件资源,实现异构机器人的自动发现、管理和调度。为了解决云边缘通信,USER引入了一个具有基于隧道的网络的自适应通信平面、用于流量本地化的分布式数据通道,以及流多处理器感知的权重同步以调节GPU端开销。在此基础设施之上,USER将学习组织为一个完全异步的框架,具有持久的、缓存感知的缓冲区,实现高效的长期实验,具有强大的崩溃恢复和历史数据的重用。此外,USER为奖励、算法和策略提供可扩展的抽象,在统一流水线内支持CNN/MLP、生成策略和大型视觉-语言-动作(VLA)模型的在线模仿或强化学习。在仿真和真实世界中的结果表明,USER实现了多机器人协调、异构机械手、具有大型模型的边缘云协作以及长期运行的异步训练,为真实世界在线策略学习提供了统一且可扩展的系统基础。"
}

def apply_translations(input_file, output_file, translations):
    """应用翻译到JSON文件"""
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    for item in data:
        if item['id'] in translations:
            item['chinese'] = translations[item['id']]
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    translated_count = len([item for item in data if item['chinese']])
    print(f"✅ 已翻译 {translated_count}/{len(data)} 篇摘要")
    return translated_count

if __name__ == "__main__":
    input_file = "/data/workspace/papers-weekly-site/abstracts.json"
    count = apply_translations(input_file, input_file, translations_batch2_3)
    print(f"📝 进度: {count}/80 ({count/80*100:.1f}%)")
