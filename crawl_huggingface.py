#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HuggingFace Papers API çˆ¬è™«
ä½¿ç”¨å®˜æ–¹APIçˆ¬å–æ¯å‘¨ç²¾é€‰è®ºæ–‡
"""

import requests
import json
from datetime import datetime

def crawl_huggingface_papers_api(week='2026-W06'):
    """ä½¿ç”¨APIçˆ¬å–HuggingFace Papers"""
    
    api_url = f'https://huggingface.co/api/daily_papers?week={week}'
    print(f"ğŸŒ æ­£åœ¨è®¿é—®API: {api_url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(api_url, headers=headers, timeout=30)
        response.raise_for_status()
        print(f"âœ… APIè¯·æ±‚æˆåŠŸ (çŠ¶æ€ç : {response.status_code})")
        
        data = response.json()
        print(f"ğŸ“¦ è·å–åˆ° {len(data)} ç¯‡è®ºæ–‡")
        
        papers = []
        
        for idx, item in enumerate(data, 1):
            try:
                paper_data = item.get('paper', {})
                
                # æå–è®ºæ–‡ID
                paper_id = paper_data.get('id', '')
                
                # æå–æ ‡é¢˜
                title = paper_data.get('title', 'Untitled')
                
                # æå–ä½œè€…
                authors_list = paper_data.get('authors', [])
                if authors_list:
                    # æå–ä½œè€…åå­—
                    author_names = []
                    for author in authors_list:
                        name = author.get('name', '')
                        if name:
                            author_names.append(name)
                    authors = ', '.join(author_names) if author_names else 'Unknown Authors'
                else:
                    authors = 'Unknown Authors'
                
                # æå–æ‘˜è¦
                summary = paper_data.get('summary', '')
                if not summary:
                    summary = "No abstract available."
                
                # æ„å»ºarXiv URL
                url = f"https://arxiv.org/abs/{paper_id}"
                
                paper = {
                    'title': title.strip(),
                    'authors': authors.strip(),
                    'summary': summary.strip(),
                    'url': url
                }
                
                papers.append(paper)
                print(f"  [{idx}] {title[:60]}...")
                
            except Exception as e:
                print(f"  âš ï¸  è§£æç¬¬{idx}æ¡æ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"\nâœ… æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
        return papers
        
    except requests.RequestException as e:
        print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
        return []
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}")
        return []
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        return []

def save_papers(papers, week='2026-W06'):
    """ä¿å­˜è®ºæ–‡åˆ°JSONæ–‡ä»¶"""
    
    output_file = f'/data/workspace/huggingface_papers_{week}.json'
    
    data = {
        'source': 'HuggingFace Papers',
        'week': week,
        'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'count': len(papers),
        'papers': papers
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
    return output_file

if __name__ == "__main__":
    week = '2026-W06'
    print(f"ğŸš€ å¼€å§‹çˆ¬å– HuggingFace Papers {week}")
    print("=" * 70)
    
    papers = crawl_huggingface_papers_api(week)
    
    if papers:
        output_file = save_papers(papers, week)
        print("\n" + "=" * 70)
        print(f"âœ… ä»»åŠ¡å®Œæˆï¼")
        print(f"ğŸ“Š å…±çˆ¬å– {len(papers)} ç¯‡è®ºæ–‡")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {output_file}")
    else:
        print("\nâŒ æœªèƒ½çˆ¬å–åˆ°è®ºæ–‡æ•°æ®")
