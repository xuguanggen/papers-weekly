#!/usr/bin/env python3
"""
æ™ºèƒ½è®ºæ–‡çˆ¬è™« - å¸¦LLMåˆ†æå’Œè¯„åˆ†
"""
import requests
import json
import time
from datetime import datetime

def fetch_weekly_papers(week):
    """ä»HuggingFaceçˆ¬å–æŒ‡å®šå‘¨çš„è®ºæ–‡"""
    print(f"ğŸ” æ­£åœ¨çˆ¬å– {week} çš„è®ºæ–‡...")
    url = f'https://huggingface.co/api/daily_papers?week={week}'
    
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        papers = response.json()
        print(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        return []

def analyze_paper_relevance(title, abstract):
    """
    åˆ†æè®ºæ–‡ä¸ç‰¹å®šé¢†åŸŸçš„ç›¸å…³åº¦
    è¿”å›: {game, efficiency, llm, agent, total}
    """
    # è¿™é‡Œæˆ‘ä»¬ä¼šç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…æ¥æ¨¡æ‹ŸLLMåˆ†æ
    # å®é™…ä½¿ç”¨æ—¶å¯ä»¥è°ƒç”¨çœŸå®çš„LLM API
    
    title_lower = title.lower()
    abstract_lower = abstract.lower()
    text = f"{title_lower} {abstract_lower}"
    
    # æ¸¸æˆç›¸å…³å…³é”®è¯
    game_keywords = [
        'game', 'gaming', 'player', 'gameplay', 'video game',
        'reinforcement learning', 'simulation', 'environment',
        'unity', 'unreal', '3d', 'virtual', 'interactive'
    ]
    
    # å·¥ç¨‹ææ•ˆå…³é”®è¯
    efficiency_keywords = [
        'efficiency', 'optimization', 'performance', 'speed',
        'acceleration', 'fast', 'efficient', 'scalable',
        'engineering', 'deployment', 'production', 'inference',
        'compilation', 'quantization', 'pruning', 'compression'
    ]
    
    # LLMç›¸å…³å…³é”®è¯
    llm_keywords = [
        'language model', 'llm', 'gpt', 'transformer', 'bert',
        'pretrain', 'fine-tuning', 'prompt', 'instruction',
        'generation', 'nlp', 'natural language', 'chat',
        'reasoning', 'understanding', 'text'
    ]
    
    # Agentç›¸å…³å…³é”®è¯
    agent_keywords = [
        'agent', 'autonomous', 'planning', 'reasoning',
        'decision making', 'tool use', 'action', 'policy',
        'multi-agent', 'collaboration', 'interaction',
        'embodied', 'robot', 'control'
    ]
    
    def calculate_score(keywords, max_score=10):
        """åŸºäºå…³é”®è¯åŒ¹é…è®¡ç®—åˆ†æ•°"""
        matches = sum(1 for kw in keywords if kw in text)
        # å½’ä¸€åŒ–åˆ°0-10åˆ†
        score = min(max_score, matches * 2)
        return score
    
    game_score = calculate_score(game_keywords)
    efficiency_score = calculate_score(efficiency_keywords)
    llm_score = calculate_score(llm_keywords)
    agent_score = calculate_score(agent_keywords)
    
    # è®¡ç®—æ€»åˆ†ï¼ˆåŠ æƒï¼‰
    total_score = (game_score * 1.0 + 
                   efficiency_score * 1.2 + 
                   llm_score * 1.5 + 
                   agent_score * 1.3)
    
    return {
        'game': round(game_score, 1),
        'efficiency': round(efficiency_score, 1),
        'llm': round(llm_score, 1),
        'agent': round(agent_score, 1),
        'total': round(total_score, 1)
    }

def process_papers(raw_papers):
    """å¤„ç†å’Œåˆ†æè®ºæ–‡"""
    processed = []
    
    for i, paper in enumerate(raw_papers, 1):
        print(f"ğŸ“Š åˆ†æè®ºæ–‡ {i}/{len(raw_papers)}: {paper.get('title', 'Unknown')[:50]}...")
        
        title = paper.get('title', '')
        abstract = paper.get('summary', '')
        
        # åˆ†æç›¸å…³åº¦
        scores = analyze_paper_relevance(title, abstract)
        
        # æ­£ç¡®æå–ä½œè€…ä¿¡æ¯ - ä» paper.authors ä¸­è·å–
        paper_obj = paper.get('paper', {})
        authors_list = paper_obj.get('authors', [])
        if authors_list:
            # æå–ä½œè€…å§“å
            authors = ', '.join([a.get('name', '') for a in authors_list[:5] if a.get('name')])
        else:
            authors = 'ä½œè€…ä¿¡æ¯å¾…è·å–'
        
        # æ­£ç¡®æå–arXiv ID - ä» paper.id ä¸­è·å–
        arxiv_id = paper_obj.get('id', '')
        if arxiv_id:
            url = f"https://arxiv.org/abs/{arxiv_id}"
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•ä»å…¶ä»–å­—æ®µè·å–
            url = paper.get('arxivId', '')
            if url and not url.startswith('http'):
                url = f"https://arxiv.org/abs/{url}"
        
        processed_paper = {
            'title': title,
            'authors': authors,
            'abstract': abstract,  # è‹±æ–‡æ‘˜è¦ï¼Œç¨åç¿»è¯‘
            'abstract_zh': '',  # å¾…ç¿»è¯‘
            'url': url,
            'publishedAt': paper.get('publishedAt', ''),
            'scores': scores
        }
        
        processed.append(processed_paper)
        time.sleep(0.1)  # é¿å…è¿‡å¿«
    
    return processed

def main():
    print("=" * 70)
    print("ğŸš€ æ™ºèƒ½è®ºæ–‡çˆ¬è™«ä¸åˆ†æç³»ç»Ÿ")
    print("=" * 70)
    
    # 1. çˆ¬å–è®ºæ–‡
    week = '2026-W07'
    raw_papers = fetch_weekly_papers(week)
    
    if not raw_papers:
        print("âŒ æ²¡æœ‰è·å–åˆ°è®ºæ–‡æ•°æ®")
        return
    
    # 2. åˆ†æè®ºæ–‡
    print(f"\nğŸ“Š å¼€å§‹åˆ†æ {len(raw_papers)} ç¯‡è®ºæ–‡...")
    processed_papers = process_papers(raw_papers)
    
    # 3. æŒ‰æ€»åˆ†é™åºæ’åº
    processed_papers.sort(key=lambda x: x['scores']['total'], reverse=True)
    
    # 4. æ·»åŠ æ’åºåçš„åºå·
    for i, paper in enumerate(processed_papers, 1):
        paper['rank'] = i
    
    # 5. ä¿å­˜ç»“æœ
    output = {
        'week': week,
        'total': len(processed_papers),
        'generated_at': datetime.now().isoformat(),
        'papers': processed_papers
    }
    
    output_file = f'/data/workspace/papers-weekly-site/archives/{week}.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # 6. æ˜¾ç¤ºTop 10
    print("\n" + "=" * 70)
    print("ğŸ† Top 10 æœ€ç›¸å…³è®ºæ–‡:")
    print("=" * 70)
    
    for paper in processed_papers[:10]:
        print(f"\n#{paper['rank']} {paper['title'][:60]}...")
        print(f"   ğŸ® æ¸¸æˆ: {paper['scores']['game']} | "
              f"âš¡ ææ•ˆ: {paper['scores']['efficiency']} | "
              f"ğŸ¤– LLM: {paper['scores']['llm']} | "
              f"ğŸ¯ Agent: {paper['scores']['agent']} | "
              f"ğŸ’¯ æ€»åˆ†: {paper['scores']['total']}")
    
    # 7. æ›´æ–°ç´¢å¼•
    index_data = {
        'archives': [
            {
                'week': week,
                'date': '2026-02-09',
                'count': len(processed_papers)
            }
        ]
    }
    
    with open('/data/workspace/papers-weekly-site/archives/index.json', 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ å¾…ç¿»è¯‘æ‘˜è¦æ•°é‡: {len(processed_papers)}")
    print("ğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡Œç¿»è¯‘è„šæœ¬å°†æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡")

if __name__ == '__main__':
    main()
