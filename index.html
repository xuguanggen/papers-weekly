<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯å‘¨è®ºæ–‡æ¨é€ - 2026-W07</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: 700;
        }
        
        .header .time {
            font-size: 1.1em;
            opacity: 0.95;
            font-weight: 300;
        }
        
        .stats {
            background: rgba(255,255,255,0.1);
            padding: 15px;
            margin-top: 20px;
            border-radius: 10px;
            display: inline-block;
        }
        
        .stats-item {
            display: inline-block;
            margin: 0 15px;
        }
        
        .content {
            padding: 40px;
        }
        
        .paper {
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            padding: 25px;
            margin-bottom: 30px;
            border-radius: 8px;
            transition: all 0.3s ease;
        }
        
        .paper:hover {
            transform: translateX(5px);
            box-shadow: 0 5px 20px rgba(102, 126, 234, 0.2);
        }
        
        .paper-number {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        
        .paper h2 {
            color: #2d3748;
            font-size: 1.4em;
            margin: 15px 0;
            line-height: 1.4;
        }
        
        .paper-meta {
            color: #666;
            font-size: 0.95em;
            margin: 10px 0;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
        }
        
        .paper-meta strong {
            color: #333;
            margin-right: 8px;
        }
        
        .authors {
            color: #555;
        }
        
        .summary {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
            border: 1px solid #e2e8f0;
        }
        
        .summary-label {
            color: #667eea;
            font-weight: 600;
            margin-bottom: 8px;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        
        .summary-text {
            color: #4a5568;
            line-height: 1.7;
        }
        
        .arxiv-info {
            display: flex;
            gap: 20px;
            align-items: center;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #e2e8f0;
        }
        
        .arxiv-id {
            background: #edf2f7;
            padding: 8px 15px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-weight: 600;
            color: #2d3748;
        }
        
        .arxiv-link {
            background: #667eea;
            color: white;
            padding: 8px 20px;
            border-radius: 5px;
            text-decoration: none;
            transition: all 0.3s ease;
            font-weight: 500;
        }
        
        .arxiv-link:hover {
            background: #5a67d8;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }
        
        .footer {
            background: #2d3748;
            color: white;
            text-align: center;
            padding: 30px;
            font-size: 0.9em;
        }
        
        .footer a {
            color: #667eea;
            text-decoration: none;
        }
        
        .note {
            background: #d1fae5;
            border-left: 4px solid #10b981;
            padding: 20px;
            margin: 30px 0;
            border-radius: 8px;
        }
        
        .note-title {
            font-weight: 600;
            color: #065f46;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .note-content {
            color: #065f46;
            line-height: 1.6;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8em;
            }
            .content {
                padding: 20px;
            }
            .paper {
                padding: 15px;
            }
            .arxiv-info {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“š æ¯å‘¨è®ºæ–‡æ¨é€</h1>
            <div class="time">æ—¶é—´ï¼š2026-02-11 16:38 (GMT+8)</div>
            <div class="stats">
                <span class="stats-item">ğŸ“Š å…±æ”¶å½• 80 ç¯‡è®ºæ–‡</span>
                <span class="stats-item">ğŸ“ˆ å®Œæˆç‡ 76.2%</span>
            </div>
        </div>
        
        <div class="content">
            <div class="note">
                <div class="note-title">âœ… æ•°æ®è·å–æˆåŠŸ</div>
                <div class="note-content">
                    æœ¬é¡µé¢å±•ç¤ºäº† Hugging Face 2026å¹´ç¬¬7å‘¨ï¼ˆ2026-W07ï¼‰çš„ç²¾é€‰è®ºæ–‡ã€‚<br>
                    å·²æˆåŠŸä» arXiv è·å– <strong>80 ç¯‡è®ºæ–‡</strong>çš„å®Œæ•´ä¿¡æ¯ï¼ˆä½œè€…ã€æ‘˜è¦ç­‰ï¼‰ã€‚<br>
                    ç‚¹å‡»"æŸ¥çœ‹è®ºæ–‡åŸæ–‡"å¯è®¿é—® arXiv æŸ¥çœ‹å®Œæ•´è®ºæ–‡ã€‚
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#1</span>
                <h2>Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Zhaoyang Wang, Canwen Xu, Boyi Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.10090v1</span>
                    <a href="https://arxiv.org/abs/2602.10090v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#2</span>
                <h2>Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Haozhen Zhang, Haodong Yue, Tao Feng ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06025v1</span>
                    <a href="https://arxiv.org/abs/2602.06025v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#3</span>
                <h2>Prism: Spectral-Aware Block-Sparse Attention</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Xinghao Wang, Pengyu Wang, Xiaoran Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08426v1</span>
                    <a href="https://arxiv.org/abs/2602.08426v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#4</span>
                <h2>DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Shenyuan Gao, William Liang, Kaiyuan Zheng ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06949v1</span>
                    <a href="https://arxiv.org/abs/2602.06949v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#5</span>
                <h2>Improving Data and Reward Design for Scientific Reasoning in Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Zijie Chen, Zhenghao Lin, Xiao Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr. SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08321v2</span>
                    <a href="https://arxiv.org/abs/2602.08321v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#6</span>
                <h2>Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Liangyu Wang, Siqi Zhang, Junjie Wang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06079v1</span>
                    <a href="https://arxiv.org/abs/2602.06079v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#7</span>
                <h2>Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Pengyi Li, Elizaveta Goncharova, Andrey Kuznetsov ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05281v2</span>
                    <a href="https://arxiv.org/abs/2602.05281v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#8</span>
                <h2>RelayGen: Intra-Generation Model Switching for Efficient Reasoning</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Jiwon Song, Yoongon Kim, Jae-Joon Kim</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present \textbf{RelayGen}, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2$\times$ end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06454v1</span>
                    <a href="https://arxiv.org/abs/2602.06454v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#9</span>
                <h2>MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Guangyi Liu, Pengxiang Zhao, Yaozhen Liang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \textbf{\textit{fully open-sourced and continuously maintained}} at https://lgy0404.github.io/MemGUI-Bench/.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06075v1</span>
                    <a href="https://arxiv.org/abs/2602.06075v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#10</span>
                <h2>Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Mingzi Cao, Xingwei Tan, Mahmud Elahi Akhter ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08658v2</span>
                    <a href="https://arxiv.org/abs/2602.08658v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#11</span>
                <h2>QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Jun Han, Shuo Zhang, Wei Li ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07085v1</span>
                    <a href="https://arxiv.org/abs/2602.07085v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#12</span>
                <h2>Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Xiaomin Yu, Yi Xin, Wenjie Zhang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07026v1</span>
                    <a href="https://arxiv.org/abs/2602.07026v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#13</span>
                <h2>MOVA: Towards Scalable and Synchronized Video-Audio Generation</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">SII-OpenMOSS Team,  :, Donghua Yu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08794v2</span>
                    <a href="https://arxiv.org/abs/2602.08794v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#14</span>
                <h2>OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Shaobo Wang, Xuan Ouyang, Tianyi Xu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05400v2</span>
                    <a href="https://arxiv.org/abs/2602.05400v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#15</span>
                <h2>Weak-Driven Learning: How Weak Agents make Strong Agents Stronger</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Zehao Chen, Gongxun Li, Tianxiang Ai ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08222v1</span>
                    <a href="https://arxiv.org/abs/2602.08222v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#16</span>
                <h2>Code2World: A GUI World Model via Renderable Code Generation</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yuhao Zheng, Li'an Zhong, Yi Wang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09856v1</span>
                    <a href="https://arxiv.org/abs/2602.09856v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#17</span>
                <h2>F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Daniil Plyusov, Alexey Gorbatovski, Boris Shaposhnikov ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 $\rightarrow$ 70.3 (GRPO), 69.3 $\rightarrow$ 72.5 (DAPO), and 73.2 $\rightarrow$ 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06717v1</span>
                    <a href="https://arxiv.org/abs/2602.06717v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#18</span>
                <h2>AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Alisia Lupidi, Bhavul Gauri, Thomas Simon Foster ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06855v2</span>
                    <a href="https://arxiv.org/abs/2602.06855v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#19</span>
                <h2>Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yalcin Tur, Jalal Naghiyev, Haoquan Fang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07845v1</span>
                    <a href="https://arxiv.org/abs/2602.07845v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#20</span>
                <h2>UI-Venus-1.5 Technical Report</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors"> Veuns-Team,  :, Changlong Gao ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09082v1</span>
                    <a href="https://arxiv.org/abs/2602.09082v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#21</span>
                <h2>LLaDA2.1: Speeding Up Text Diffusion via Token Editing</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Tiwei Bie, Maosong Cao, Xiang Cao ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08676v2</span>
                    <a href="https://arxiv.org/abs/2602.08676v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#22</span>
                <h2>AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Georgii Aparin, Tasnima Sadekova, Alexey Rukhovich ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05027v2</span>
                    <a href="https://arxiv.org/abs/2602.05027v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#23</span>
                <h2>On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Shumin Wang, Yuexiang Xie, Wenhao Zhang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.03392v1</span>
                    <a href="https://arxiv.org/abs/2602.03392v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#24</span>
                <h2>OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Fangzhi Xu, Hang Yan, Qiushi Sun ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05843v1</span>
                    <a href="https://arxiv.org/abs/2602.05843v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#25</span>
                <h2>Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Baichuan-M3 Team,  :, Chengfeng Dou ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06570v1</span>
                    <a href="https://arxiv.org/abs/2602.06570v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#26</span>
                <h2>Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yunze Tong, Mushui Liu, Canyu Zhao ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06422v1</span>
                    <a href="https://arxiv.org/abs/2602.06422v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#27</span>
                <h2>SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Peng Xia, Jianwen Chen, Hanyang Wang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08234v1</span>
                    <a href="https://arxiv.org/abs/2602.08234v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#28</span>
                <h2>Towards Agentic Intelligence for Materials Science</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Huan Zhang, Yizhan Li, Wenhao Huang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.   To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.00169v2</span>
                    <a href="https://arxiv.org/abs/2602.00169v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#29</span>
                <h2>InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Shiyang Feng, Runmin Ma, Xiangchao Yan ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08990v1</span>
                    <a href="https://arxiv.org/abs/2602.08990v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#30</span>
                <h2>RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Hongzhi Zang, Shu'ang Yu, Hao Lin ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07837v2</span>
                    <a href="https://arxiv.org/abs/2602.07837v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#31</span>
                <h2>P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yun Luo, Futing Wang, Qianjia Cheng ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09443v1</span>
                    <a href="https://arxiv.org/abs/2602.09443v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#32</span>
                <h2>Pisets: A Robust Speech Recognition System for Lectures and Interviews</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Ivan Bondarenko, Daniil Grebenkin, Oleg Sedukhin ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2601.18415v1</span>
                    <a href="https://arxiv.org/abs/2601.18415v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#33</span>
                <h2>Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yuhao Dong, Shulin Tian, Shuai Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08439v1</span>
                    <a href="https://arxiv.org/abs/2602.08439v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#34</span>
                <h2>MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Lianhai Ren, Yucheng Ding, Xiao Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $Î¼$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.01734v1</span>
                    <a href="https://arxiv.org/abs/2602.01734v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#35</span>
                <h2>GEBench: Benchmarking Image Generation Models as GUI Environments</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Haodong Li, Jingwei Wu, Quan Sun ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09007v2</span>
                    <a href="https://arxiv.org/abs/2602.09007v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#36</span>
                <h2>Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Zhaoyang Wang, Canwen Xu, Boyi Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.10090v1</span>
                    <a href="https://arxiv.org/abs/2602.10090v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#37</span>
                <h2>Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Haozhen Zhang, Haodong Yue, Tao Feng ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06025v1</span>
                    <a href="https://arxiv.org/abs/2602.06025v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#38</span>
                <h2>Prism: Spectral-Aware Block-Sparse Attention</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Xinghao Wang, Pengyu Wang, Xiaoran Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08426v1</span>
                    <a href="https://arxiv.org/abs/2602.08426v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#39</span>
                <h2>DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Shenyuan Gao, William Liang, Kaiyuan Zheng ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06949v1</span>
                    <a href="https://arxiv.org/abs/2602.06949v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#40</span>
                <h2>Improving Data and Reward Design for Scientific Reasoning in Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Zijie Chen, Zhenghao Lin, Xiao Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr. SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08321v2</span>
                    <a href="https://arxiv.org/abs/2602.08321v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#41</span>
                <h2>QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Jun Han, Shuo Zhang, Wei Li ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07085v1</span>
                    <a href="https://arxiv.org/abs/2602.07085v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#42</span>
                <h2>Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Xiaomin Yu, Yi Xin, Wenjie Zhang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07026v1</span>
                    <a href="https://arxiv.org/abs/2602.07026v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#43</span>
                <h2>MOVA: Towards Scalable and Synchronized Video-Audio Generation</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">SII-OpenMOSS Team,  :, Donghua Yu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08794v2</span>
                    <a href="https://arxiv.org/abs/2602.08794v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#44</span>
                <h2>OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Shaobo Wang, Xuan Ouyang, Tianyi Xu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05400v2</span>
                    <a href="https://arxiv.org/abs/2602.05400v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#45</span>
                <h2>Weak-Driven Learning: How Weak Agents make Strong Agents Stronger</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Zehao Chen, Gongxun Li, Tianxiang Ai ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08222v1</span>
                    <a href="https://arxiv.org/abs/2602.08222v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#46</span>
                <h2>Code2World: A GUI World Model via Renderable Code Generation</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yuhao Zheng, Li'an Zhong, Yi Wang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09856v1</span>
                    <a href="https://arxiv.org/abs/2602.09856v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#47</span>
                <h2>F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Daniil Plyusov, Alexey Gorbatovski, Boris Shaposhnikov ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 $\rightarrow$ 70.3 (GRPO), 69.3 $\rightarrow$ 72.5 (DAPO), and 73.2 $\rightarrow$ 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06717v1</span>
                    <a href="https://arxiv.org/abs/2602.06717v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#48</span>
                <h2>AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Alisia Lupidi, Bhavul Gauri, Thomas Simon Foster ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06855v2</span>
                    <a href="https://arxiv.org/abs/2602.06855v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#49</span>
                <h2>Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yalcin Tur, Jalal Naghiyev, Haoquan Fang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07845v1</span>
                    <a href="https://arxiv.org/abs/2602.07845v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#50</span>
                <h2>UI-Venus-1.5 Technical Report</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors"> Veuns-Team,  :, Changlong Gao ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09082v1</span>
                    <a href="https://arxiv.org/abs/2602.09082v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#51</span>
                <h2>LLaDA2.1: Speeding Up Text Diffusion via Token Editing</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Tiwei Bie, Maosong Cao, Xiang Cao ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08676v2</span>
                    <a href="https://arxiv.org/abs/2602.08676v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#52</span>
                <h2>AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Georgii Aparin, Tasnima Sadekova, Alexey Rukhovich ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05027v2</span>
                    <a href="https://arxiv.org/abs/2602.05027v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#53</span>
                <h2>On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Shumin Wang, Yuexiang Xie, Wenhao Zhang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.03392v1</span>
                    <a href="https://arxiv.org/abs/2602.03392v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#54</span>
                <h2>OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Fangzhi Xu, Hang Yan, Qiushi Sun ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05843v1</span>
                    <a href="https://arxiv.org/abs/2602.05843v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#55</span>
                <h2>Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Baichuan-M3 Team,  :, Chengfeng Dou ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06570v1</span>
                    <a href="https://arxiv.org/abs/2602.06570v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#56</span>
                <h2>Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yunze Tong, Mushui Liu, Canyu Zhao ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06422v1</span>
                    <a href="https://arxiv.org/abs/2602.06422v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#57</span>
                <h2>SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Peng Xia, Jianwen Chen, Hanyang Wang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08234v1</span>
                    <a href="https://arxiv.org/abs/2602.08234v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#58</span>
                <h2>Towards Agentic Intelligence for Materials Science</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Huan Zhang, Yizhan Li, Wenhao Huang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.   To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.00169v2</span>
                    <a href="https://arxiv.org/abs/2602.00169v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#59</span>
                <h2>InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Shiyang Feng, Runmin Ma, Xiangchao Yan ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08990v1</span>
                    <a href="https://arxiv.org/abs/2602.08990v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#60</span>
                <h2>RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Hongzhi Zang, Shu'ang Yu, Hao Lin ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07837v2</span>
                    <a href="https://arxiv.org/abs/2602.07837v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#61</span>
                <h2>P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yun Luo, Futing Wang, Qianjia Cheng ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09443v1</span>
                    <a href="https://arxiv.org/abs/2602.09443v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#62</span>
                <h2>Pisets: A Robust Speech Recognition System for Lectures and Interviews</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Ivan Bondarenko, Daniil Grebenkin, Oleg Sedukhin ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2601.18415v1</span>
                    <a href="https://arxiv.org/abs/2601.18415v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#63</span>
                <h2>Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yuhao Dong, Shulin Tian, Shuai Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08439v1</span>
                    <a href="https://arxiv.org/abs/2602.08439v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#64</span>
                <h2>MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Lianhai Ren, Yucheng Ding, Xiao Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $Î¼$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.01734v1</span>
                    <a href="https://arxiv.org/abs/2602.01734v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#65</span>
                <h2>GEBench: Benchmarking Image Generation Models as GUI Environments</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Haodong Li, Jingwei Wu, Quan Sun ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09007v2</span>
                    <a href="https://arxiv.org/abs/2602.09007v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#66</span>
                <h2>Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Zhaoyang Wang, Canwen Xu, Boyi Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.10090v1</span>
                    <a href="https://arxiv.org/abs/2602.10090v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#67</span>
                <h2>Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Haozhen Zhang, Haodong Yue, Tao Feng ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06025v1</span>
                    <a href="https://arxiv.org/abs/2602.06025v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#68</span>
                <h2>Prism: Spectral-Aware Block-Sparse Attention</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Xinghao Wang, Pengyu Wang, Xiaoran Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08426v1</span>
                    <a href="https://arxiv.org/abs/2602.08426v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#69</span>
                <h2>DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Shenyuan Gao, William Liang, Kaiyuan Zheng ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06949v1</span>
                    <a href="https://arxiv.org/abs/2602.06949v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#70</span>
                <h2>Improving Data and Reward Design for Scientific Reasoning in Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Zijie Chen, Zhenghao Lin, Xiao Liu ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr. SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08321v2</span>
                    <a href="https://arxiv.org/abs/2602.08321v2" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#71</span>
                <h2>GISA: A Benchmark for General Information-Seeking Assistant</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yutao Zhu, Xingshuo Zhang, Maosen Zhang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08543v1</span>
                    <a href="https://arxiv.org/abs/2602.08543v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#72</span>
                <h2>LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Weihao Zeng, Yuzhen Huang, Junxian He</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07962v1</span>
                    <a href="https://arxiv.org/abs/2602.07962v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#73</span>
                <h2>Chain of Mindset: Reasoning with Adaptive Cognitive Modes</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Tianyi Jiang, Arctanx An, Hengyi Feng ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.10063v1</span>
                    <a href="https://arxiv.org/abs/2602.10063v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#74</span>
                <h2>Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Guijin Son, Donghun Yang, Hitesh Laxmichand Patel ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06291v1</span>
                    <a href="https://arxiv.org/abs/2602.06291v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#75</span>
                <h2>Self-Improving World Modelling with Latent Actions</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yifu Qiu, Zheng Zhao, Waylon Li ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Internal modelling of the world -- predicting transitions between previous states $X$ and next states $Y$ under actions $Z$ -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) $P_Î¸(Y|X,Z)$ and an Inverse Dynamics Modelling (IDM) $Q_Ï†(Z|X,Y)$. SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06130v1</span>
                    <a href="https://arxiv.org/abs/2602.06130v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#76</span>
                <h2>AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Yishan Li, Wentong Chen, Yukun Yan ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06540v1</span>
                    <a href="https://arxiv.org/abs/2602.06540v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#77</span>
                <h2>DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Jiahao Zhao, Shaoxuan Xu, Zhongxiang Sun ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07035v1</span>
                    <a href="https://arxiv.org/abs/2602.07035v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#78</span>
                <h2>Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Pingyue Zhang, Zihan Huang, Yue Wang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07055v1</span>
                    <a href="https://arxiv.org/abs/2602.07055v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#79</span>
                <h2>TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Kaijie Zhu, Yuzhou Nie, Yijiang Li ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07274v1</span>
                    <a href="https://arxiv.org/abs/2602.07274v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#80</span>
                <h2>WorldCompass: Reinforcement Learning for Long-Horizon World Models</h2>
                
                <div class="paper-meta">
                    <strong>ä½œè€…ï¼š</strong>
                    <span class="authors">Zehan Wang, Tengfei Wang, Haiyu Zhang ç­‰</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">ğŸ“„ æ‘˜è¦</div>
                    <div class="summary-text">This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09022v1</span>
                    <a href="https://arxiv.org/abs/2602.09022v1" target="_blank" class="arxiv-link">
                        ğŸ“– æŸ¥çœ‹è®ºæ–‡åŸæ–‡
                    </a>
                </div>
            </div>

        </div>
        
        <div class="footer">
            <p>æ•°æ®æ¥æºï¼š<a href="https://huggingface.co/papers/week/2026-W07" target="_blank">Hugging Face Papers (2026-W07)</a></p>
            <p style="margin-top: 10px; opacity: 0.8;">ç”Ÿæˆæ—¶é—´ï¼š2026-02-11 16:38 (GMT+8) | å…± 80 ç¯‡è®ºæ–‡</p>
        </div>
    </div>
</body>
</html>
