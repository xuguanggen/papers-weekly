<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>每周论文推送 - 2026-W07</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: 700;
        }
        
        .header .time {
            font-size: 1.1em;
            opacity: 0.95;
            font-weight: 300;
        }
        
        .stats {
            background: rgba(255,255,255,0.1);
            padding: 15px;
            margin-top: 20px;
            border-radius: 10px;
            display: inline-block;
        }
        
        .stats-item {
            display: inline-block;
            margin: 0 15px;
        }
        
        .content {
            padding: 40px;
        }
        
        .paper {
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            padding: 25px;
            margin-bottom: 30px;
            border-radius: 8px;
            transition: all 0.3s ease;
        }
        
        .paper:hover {
            transform: translateX(5px);
            box-shadow: 0 5px 20px rgba(102, 126, 234, 0.2);
        }
        
        .paper-number {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        
        .paper h2 {
            color: #2d3748;
            font-size: 1.4em;
            margin: 15px 0;
            line-height: 1.4;
        }
        
        .paper-meta {
            color: #666;
            font-size: 0.95em;
            margin: 10px 0;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
        }
        
        .paper-meta strong {
            color: #333;
            margin-right: 8px;
        }
        
        .authors {
            color: #555;
        }
        
        .summary {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
            border: 1px solid #e2e8f0;
        }
        
        .summary-label {
            color: #667eea;
            font-weight: 600;
            margin-bottom: 8px;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        
        .summary-text {
            color: #4a5568;
            line-height: 1.7;
        }
        
        .arxiv-info {
            display: flex;
            gap: 20px;
            align-items: center;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #e2e8f0;
        }
        
        .arxiv-id {
            background: #edf2f7;
            padding: 8px 15px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-weight: 600;
            color: #2d3748;
        }
        
        .arxiv-link {
            background: #667eea;
            color: white;
            padding: 8px 20px;
            border-radius: 5px;
            text-decoration: none;
            transition: all 0.3s ease;
            font-weight: 500;
        }
        
        .arxiv-link:hover {
            background: #5a67d8;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }
        
        .footer {
            background: #2d3748;
            color: white;
            text-align: center;
            padding: 30px;
            font-size: 0.9em;
        }
        
        .footer a {
            color: #667eea;
            text-decoration: none;
        }
        
        .note {
            background: #d1fae5;
            border-left: 4px solid #10b981;
            padding: 20px;
            margin: 30px 0;
            border-radius: 8px;
        }
        
        .note-title {
            font-weight: 600;
            color: #065f46;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .note-content {
            color: #065f46;
            line-height: 1.6;
        }
        
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8em;
            }
            .content {
                padding: 20px;
            }
            .paper {
                padding: 15px;
            }
            .arxiv-info {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>📚 每周论文推送</h1>
            <div class="time">时间：2026-02-11 16:38 (GMT+8)</div>
            <div class="stats">
                <span class="stats-item">📊 共收录 80 篇论文</span>
                <span class="stats-item">📈 完成率 76.2%</span>
            </div>
        </div>
        
        <div class="content">
            <div class="note">
                <div class="note-title">✅ 数据获取成功</div>
                <div class="note-content">
                    本页面展示了 Hugging Face 2026年第7周（2026-W07）的精选论文。<br>
                    已成功从 arXiv 获取 <strong>80 篇论文</strong>的完整信息（作者、摘要等）。<br>
                    点击"查看论文原文"可访问 arXiv 查看完整论文。
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#1</span>
                <h2>Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Zhaoyang Wang, Canwen Xu, Boyi Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">近年来大语言模型(LLM)的进步使自主智能体能够执行需要与工具和环境进行多轮交互的复杂任务。然而,这类智能体训练的规模化受到缺乏多样化和可靠环境的限制。在本文中,我们提出了智能体世界模型(AWM),一个完全合成的环境生成流水线。使用此流水线,我们扩展到1000个涵盖日常场景的环境,智能体可以在其中与丰富的工具集(平均每个环境35个工具)交互并获得高质量的观察。值得注意的是,这些环境由代码驱动并由数据库支持,相比LLM模拟的环境提供更可靠和一致的状态转换。此外,与从真实环境中收集轨迹相比,它们实现了更高效的智能体交互。为了展示此资源的有效性,我们对多轮工具使用智能体进行了大规模强化学习。得益于完全可执行的环境和可访问的数据库状态,我们还可以设计可靠的奖励函数。在三个基准测试上的实验表明,在合成环境而非特定基准环境中独家训练,可以产生强大的分布外泛化能力。代码可在https://github.com/Snowflake-Labs/agent-world-model获取。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.10090v1</span>
                    <a href="https://arxiv.org/abs/2602.10090v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#2</span>
                <h2>Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Haozhen Zhang, Haodong Yue, Tao Feng 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">记忆对于在单个上下文窗口之外运行的大语言模型(LLM)智能体越来越重要,但大多数现有系统依赖离线、与查询无关的记忆构建,这可能效率低下并可能丢弃查询关键信息。尽管运行时记忆利用是一个自然的替代方案,但先前的工作通常会产生大量开销,并且对性能-成本权衡的显式控制有限。在这项工作中,我们提出了BudgetMem,一个用于显式、查询感知的性能-成本控制的运行时智能体记忆框架。BudgetMem将记忆处理结构化为一组记忆模块,每个模块提供三个预算层级(即低/中/高)。一个轻量级路由器执行跨模块的预算层级路由,以平衡任务性能和记忆构建成本,这是通过强化学习训练的紧凑神经策略实现的。使用BudgetMem作为统一测试平台,我们研究了三种互补策略来实现预算层级:实现(方法复杂性)、推理(推理行为)和容量(模块模型大小)。在LoCoMo、LongMemEval和HotpotQA上,BudgetMem在优先考虑性能时(即高预算设置)超越了强基线,并在更紧预算下提供更好的准确性-成本前沿。此外,我们的分析解开了不同分层策略的优势和劣势,阐明了在不同预算制度下每个轴何时提供最有利的权衡。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06025v1</span>
                    <a href="https://arxiv.org/abs/2602.06025v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#3</span>
                <h2>Prism: Spectral-Aware Block-Sparse Attention</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Xinghao Wang, Pengyu Wang, Xiaoran Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">块稀疏注意力对于加速长上下文LLM预填充很有前景,但有效识别相关块仍然是一个瓶颈。现有方法通常采用粗粒度注意力作为块重要性估计的代理,但经常求助于昂贵的token级搜索或评分,导致显著的选择开销。在这项工作中,我们追溯了通过均值池化的标准粗粒度注意力的不准确性到理论根本原因:均值池化和旋转位置嵌入(RoPE)之间的相互作用。我们证明均值池化充当低通滤波器,在高频维度中引起破坏性干扰,有效地为局部位置信息(例如斜杠模式)创建"盲点"。为了解决这个问题,我们引入了Prism,一种无需训练的频谱感知方法,将块选择分解为高频和低频分支。通过应用基于能量的温度校准,Prism直接从池化表示中恢复衰减的位置信号,使用纯块级操作实现块重要性估计,从而提高效率。广泛的评估证实,Prism在保持与完全注意力准确性一致的同时,提供高达5.1倍的加速。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08426v1</span>
                    <a href="https://arxiv.org/abs/2602.08426v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#4</span>
                <h2>DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Shenyuan Gao, William Liang, Kaiyuan Zheng 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">能够在各种环境中模拟动作结果将彻底改变大规模通用智能体的开发。然而,建模这些世界动态,特别是对于灵巧的机器人任务,由于数据覆盖有限和动作标签稀缺而面临重大挑战。作为实现这一目标的努力,我们引入了DreamDojo,一个从44000小时的自我中心人类视频中学习多样化交互和灵巧控制的基础世界模型。我们的数据混合代表迄今为止最大的世界模型预训练视频数据集,涵盖各种日常场景,包含多样化的对象和技能。为了解决动作标签的稀缺性,我们引入连续潜在动作作为统一代理动作,增强从未标记视频的交互知识转移。在小规模目标机器人数据上进行后训练后,DreamDojo展示了对物理的强大理解和精确的动作可控性。我们还设计了一个蒸馏流水线,将DreamDojo加速到10.81 FPS的实时速度,并进一步提高上下文一致性。我们的工作实现了几个基于生成式世界模型的重要应用,包括实时远程操作、策略评估和基于模型的规划。在多个具有挑战性的分布外(OOD)基准测试上的系统评估验证了我们的方法对于模拟开放世界、接触丰富任务的重要性,为通用机器人世界模型铺平了道路。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06949v1</span>
                    <a href="https://arxiv.org/abs/2602.06949v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#5</span>
                <h2>Improving Data and Reward Design for Scientific Reasoning in Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Zijie Chen, Zhenghao Lin, Xiao Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">解决开放式科学问题对大语言模型仍然具有挑战性,特别是由于固有的不可靠监督和评估。瓶颈在于科学后训练的数据构建和奖励设计。我们开发了一个大规模、系统的数据处理流水线,将异构开源科学数据转换为Dr. SCI数据集,该数据集包含八个STEM学科的100万个问题,具有明确的可验证/开放式拆分、可扩展的难度注释,以及将开放式答案的评估操作化的细粒度评分标准。基于此数据集,我们提出了Dr. SCI后训练流水线,通过三个组件重新设计了标准的SFT -> RL工作流:(i)探索扩展SFT,在RL之前扩展模型的推理模式覆盖范围;(ii)动态难度课程,使训练数据适应模型不断演变的科学能力;(iii)SciRubric引导的RL,通过基于评分标准的评估和明确的答案正确性,实现对开放式科学问题的稳定强化学习。使用Dr. SCI流水线训练的Qwen3-4B-Base在GPQA-diamond上达到63.2,在GPQA-general上达到32.4,持续改进优于o1-mini和GPT-4o等强大的后训练基线,在科学推理方面展示了实质性收益,特别是在开放式设置中。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08321v2</span>
                    <a href="https://arxiv.org/abs/2602.08321v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#6</span>
                <h2>Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Liangyu Wang, Siqi Zhang, Junjie Wang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">大语言模型(LLM)的规模化推动了对基于矩阵的优化器(如Shampoo、Muon、SOAP)的兴趣,因为它们具有收敛效率;然而,它们对整体更新的要求与Megatron等分布式框架中的张量碎片化相冲突。现有解决方案是次优的:同步方法存在计算冗余,而逐层分区在不违反高效通信原语的几何约束的情况下无法调和这种冲突。为了弥合这一差距,我们提出了Canzona,一个统一、异步和负载平衡的框架,将逻辑优化器分配与物理参数分布解耦。对于数据并行,我们引入了一种α平衡静态分区策略,在尊重原子性的同时中和负载不平衡。对于张量并行,我们设计了一个异步计算流水线,利用微组调度来批处理碎片更新并隐藏重建开销。在256个GPU上对Qwen3模型系列(高达32B参数)的广泛评估表明,我们的方法保留了已建立的并行架构的效率,与基线相比,端到端迭代时间加速1.57倍,优化器步骤延迟减少5.8倍。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06079v1</span>
                    <a href="https://arxiv.org/abs/2602.06079v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#7</span>
                <h2>Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Pengyi Li, Elizaveta Goncharova, Andrey Kuznetsov 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">具有可验证奖励的强化学习(RLVR)已成为增强大语言模型(LLM)推理的不可或缺的范式。然而,标准策略优化方法,如群组相对策略优化(GRPO),经常收敛到低熵策略,导致严重的模式崩溃和有限的输出多样性。我们从采样概率动态的角度分析这个问题,识别出标准目标不成比例地强化最高可能性路径,从而抑制有效的替代推理链。为了解决这个问题,我们提出了一种新颖的优势重加权机制(ARM),旨在平衡所有正确响应的置信水平。通过将提示困惑度和答案置信度纳入优势估计,我们的方法动态重塑奖励信号,以衰减过度自信推理路径的梯度更新,同时将概率质量重新分配给探索不足的正确解决方案。实证结果表明,我们的方法在保持竞争准确性的同时显著增强了生成多样性和响应熵,有效地在推理任务中实现了探索和利用之间的卓越权衡。在Qwen2.5和DeepSeek模型上跨数学和编码基准的实证结果表明,ProGRPO显著缓解了熵崩溃。具体而言,在Qwen2.5-7B上,我们的方法在Pass@1上比GRPO提高了5.7%,值得注意的是,在Pass@32上提高了13.9%,突出了其在生成多样化正确推理路径方面的卓越能力。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05281v2</span>
                    <a href="https://arxiv.org/abs/2602.05281v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#8</span>
                <h2>RelayGen: Intra-Generation Model Switching for Efficient Reasoning</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Jiwon Song, Yoongon Kim, Jae-Joon Kim</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">大型推理模型(LRM)通过生成长的、多步骤的推理轨迹在复杂推理任务上取得了强大的性能,但推理时扩展会产生大量部署成本。一个关键挑战是生成难度在单个输出内变化,而现有的面向效率的方法要么忽略这种生成内变化,要么依赖于具有高系统复杂性的监督token级路由。我们提出了RelayGen,一个无需训练的、段级运行时模型切换框架,利用长形式推理中的难度变化。通过使用token概率边际对生成不确定性进行离线分析,我们表明粗粒度段级控制足以捕获推理轨迹内的难度转换。RelayGen识别特定于模型的切换提示,这些提示信号转换到较低难度段,并动态将其继续委托给较小的模型,同时在大模型上保留高难度推理。在多个推理基准测试中,RelayGen在保留大模型大部分准确性的同时大幅减少了推理延迟。当与推测解码结合使用时,RelayGen实现了高达2.2倍的端到端加速,准确性降低不到2%,无需额外训练或学习路由组件。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06454v1</span>
                    <a href="https://arxiv.org/abs/2602.06454v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#9</span>
                <h2>MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Guangyi Liu, Pengxiang Zhao, Yaozhen Liang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">当前的移动GUI智能体基准系统性地无法评估记忆能力,只有5.2-11.8%的记忆相关任务,并且没有跨会话学习评估。我们引入了MemGUI-Bench,一个全面的以记忆为中心的基准,具有pass@k和分阶段的LLM作为裁判评估。我们的贡献包括:(1)一个系统的记忆分类法,分析5种架构的11个智能体;(2)26个应用程序中的128个任务,其中89.8%通过跨时间和跨空间保留来挑战记忆;(3)MemGUI-Eval,一个具有渐进审查和7个分层指标的自动化流水线;(4)对11个最先进智能体的RQ驱动评估。我们的实验揭示了所有评估系统中的显著记忆缺陷,识别了5种不同的失败模式,并综合了5个可操作的设计含义。所有资源包括代码、基准和评估结果将在https://lgy0404.github.io/MemGUI-Bench/完全开源并持续维护。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06075v1</span>
                    <a href="https://arxiv.org/abs/2602.06075v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#10</span>
                <h2>Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Mingzi Cao, Xingwei Tan, Mahmud Elahi Akhter 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">演绎、归纳和溯因是基本的推理范式,是人类逻辑思维的核心。尽管改进大语言模型(LLM)推理已经吸引了大量研究努力,但基本范式诱导泛化的程度尚未得到系统探索。在这项研究中,我们阐明了这些核心范式之间的相互作用如何影响LLM的推理行为。为此,我们首先从符号任务中收集了一个新的推理轨迹数据集,每个任务针对三个基本范式之一,以从具体的世界知识中抽象出来。然后,我们研究将这些技能诱导到LLM中的有效方法。我们使用一系列方法进行实验,包括简单的微调,以及更复杂的方法来增加模型深度,或将密集模型转换为混合专家。我们在完全用自然语言表述并包含真实世界知识的现实分布外任务上全面评估诱导模型。我们的结果表明,我们的方法在现实任务中产生了强大的泛化能力,性能提升显著(高达14.60)。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08658v2</span>
                    <a href="https://arxiv.org/abs/2602.08658v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#11</span>
                <h2>QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Jun Han, Shuo Zhang, Wei Li 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">金融市场嘈杂且非平稳,使得阿尔法挖掘对回测结果中的噪声和突然的市场制度转变高度敏感。虽然最近的智能体框架改进了阿尔法挖掘自动化,但它们通常缺乏可控的多轮搜索和可靠的经验重用。为了应对这些挑战,我们提出了QuantaAlpha,一个进化的阿尔法挖掘框架,将每次端到端挖掘运行视为一个轨迹,并通过轨迹级突变和交叉操作改进因子。QuantaAlpha定位每个轨迹中的次优步骤进行针对性修订,并重新组合互补的高奖励片段以重用有效模式,实现跨挖掘迭代的结构化探索和细化。在因子生成过程中,QuantaAlpha在假设、因子表达和可执行代码之间强制语义一致性,同时约束生成因子的复杂性和冗余以缓解拥挤。在中证300指数(CSI 300)上的广泛实验展示了相对于强基线模型和先前智能体系统的一致收益。当使用GPT-5.2时,QuantaAlpha实现0.1501的信息系数(IC)、27.75%的年化收益率(ARR)和7.98%的最大回撤(MDD)。此外,在CSI 300上挖掘的因子有效转移到中证500指数(CSI 500)和标准普尔500指数(S&P 500),在四年内分别提供160%和137%的累计超额回报,这表明QuantaAlpha在市场分布转变下具有强大的鲁棒性。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07085v1</span>
                    <a href="https://arxiv.org/abs/2602.07085v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#12</span>
                <h2>Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Xiaomin Yu, Yi Xin, Wenjie Zhang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">尽管多模态对比学习在对齐视觉和语言表示方面取得了成功,但一个持续存在的几何异常,即模态差距仍然存在:表达相同语义的不同模态的嵌入占据系统偏移的区域。弥合这一差距的先前方法在很大程度上受到过度简化的各向同性假设的限制,阻碍了它们在大规模场景中的应用。在本文中,我们通过精确表征模态差距的几何形状并利用它进行高效的模型扩展来解决这些限制。首先,我们提出固定框架模态差距理论,该理论在冻结的参考框架内将模态差距分解为稳定偏差和各向异性残差。在这种精确建模的指导下,我们引入了ReAlign,一种无需训练的模态对齐策略。利用来自大量未配对数据的统计信息,ReAlign通过包括锚点、轨迹和质心对齐的三步过程将文本表示对齐到图像表示分布中,从而明确纠正几何错位。基于ReAlign,我们提出了ReVision,一个用于多模态大语言模型(MLLM)的可扩展训练范式。ReVision将ReAlign集成到预训练阶段,使模型能够在视觉指令调优之前从未配对文本中学习视觉表示的分布,而无需大规模、高质量的图像-文本对。我们的框架展示了统计对齐的未配对数据可以有效替代昂贵的图像-文本对,为MLLM的高效扩展提供了强大的路径。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07026v1</span>
                    <a href="https://arxiv.org/abs/2602.07026v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#13</span>
                <h2>MOVA: Towards Scalable and Synchronized Video-Audio Generation</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">SII-OpenMOSS Team,  :, Donghua Yu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">音频对于真实世界的视频是不可或缺的,但生成模型在很大程度上忽视了音频组件。当前生成视听内容的方法通常依赖级联流水线,这增加了成本、累积错误并降低了整体质量。虽然Veo 3和Sora 2等系统强调同时生成的价值,但联合多模态建模在架构、数据和训练方面带来了独特的挑战。此外,现有系统的闭源性质限制了该领域的进展。在这项工作中,我们引入了MOVA(MOSS视频和音频),一个能够生成高质量、同步视听内容的开源模型,包括逼真的唇音同步语音、环境感知音效和内容对齐音乐。MOVA采用混合专家(MoE)架构,总共有32B参数,其中18B在推理过程中处于活动状态。它支持IT2VA(图像-文本到视频-音频)生成任务。通过发布模型权重和代码,我们旨在推进研究并培养一个充满活力的创作者社区。发布的代码库为高效推理、LoRA微调和提示增强提供全面支持。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08794v2</span>
                    <a href="https://arxiv.org/abs/2602.08794v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#14</span>
                <h2>OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Shaobo Wang, Xuan Ouyang, Tianyi Xu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">随着高质量公共文本接近枯竭,这一现象被称为数据墙,预训练正在从更多token转向更好的token。然而,现有方法要么依赖忽略训练动态的启发式静态过滤器,要么使用基于原始梯度的动态但与优化器无关的标准。我们提出OPUS(优化器诱导的投影效用选择),一个在优化器诱导的更新空间中定义效用的动态数据选择框架。OPUS通过将由现代优化器塑造的有效更新投影到从稳定的分布内代理派生的目标方向上来对候选者进行评分。为确保可扩展性,我们使用CountSketch的Ghost技术实现计算效率,并使用Boltzmann采样实现数据多样性,仅产生4.7%的额外计算开销。OPUS在各种语料库、质量层级、优化器和模型规模上取得了显著成果。在FineWeb和FineWeb-Edu上使用30B token对GPT-2 Large/XL进行预训练时,OPUS优于工业级基线,甚至超过完整的200B token训练。此外,当与工业级静态过滤器结合使用时,OPUS进一步提高了预训练效率,即使使用较低质量的数据。此外,在SciencePedia上对Qwen3-8B-Base进行持续预训练时,OPUS仅使用0.5B token就实现了优于使用3B token的完整训练的卓越性能,展示了在专业领域的显著数据效率收益。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05400v2</span>
                    <a href="https://arxiv.org/abs/2602.05400v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#15</span>
                <h2>Weak-Driven Learning: How Weak Agents make Strong Agents Stronger</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Zehao Chen, Gongxun Li, Tianxiang Ai 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">随着后训练优化成为改进大语言模型的核心,我们观察到一个持续存在的饱和瓶颈:一旦模型变得高度自信,进一步的训练产生递减的回报。虽然现有方法继续强化目标预测,但我们发现信息性监督信号仍然潜伏在模型自身的历史弱状态中。受此观察的启发,我们提出了WMSS(弱智能体可以使强智能体更强),一个利用弱检查点指导持续优化的后训练范式。通过熵动态识别可恢复的学习差距并通过补偿学习强化它们,WMSS使强智能体能够超越传统的后训练饱和。在数学推理和代码生成数据集上的实验表明,使用我们方法训练的智能体实现了有效的性能改进,同时产生零额外推理成本。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08222v1</span>
                    <a href="https://arxiv.org/abs/2602.08222v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#16</span>
                <h2>Code2World: A GUI World Model via Renderable Code Generation</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yuhao Zheng, Li'an Zhong, Yi Wang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">自主GUI智能体通过感知界面和执行动作与环境交互。作为虚拟沙箱,GUI世界模型通过启用动作条件预测为智能体赋予类似人类的远见。然而,现有的基于文本和像素的方法难以同时实现高视觉保真度和细粒度结构可控性。为此,我们提出Code2World,一个通过可渲染代码生成模拟下一个视觉状态的视觉-语言编码器。具体来说,为了解决数据稀缺问题,我们通过将GUI轨迹转换为高保真HTML并通过视觉反馈修订机制完善合成代码来构建AndroidCode,产生超过80K个高质量屏幕-动作对的语料库。为了使现有VLM适应代码预测,我们首先执行SFT作为格式布局遵循的冷启动,然后进一步应用渲染感知强化学习,该学习使用渲染结果作为奖励信号,通过强制视觉语义保真度和动作一致性。广泛的实验表明,Code2World-8B实现了顶级的下一个UI预测,与竞争激烈的GPT-5和Gemini-3-Pro-Image相媲美。值得注意的是,Code2World以灵活的方式显著提高了下游导航成功率,在AndroidWorld导航上将Gemini-2.5-Flash提高了+9.5%。代码可在https://github.com/AMAP-ML/Code2World获取。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09856v1</span>
                    <a href="https://arxiv.org/abs/2602.09856v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#17</span>
                <h2>F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Daniil Plyusov, Alexey Gorbatovski, Boris Shaposhnikov 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">具有可验证奖励的强化学习(RLVR)通常基于群组采样来估计优势并稳定策略更新。实际上,由于计算限制,大群组规模是不可行的,这使学习偏向于已经可能的轨迹。较小的群组经常错过罕见正确的轨迹,同时仍然包含混合奖励,将概率集中在常见解决方案上。我们推导出更新错过罕见正确模式的概率作为群组大小的函数,显示非单调行为,并表征更新如何在正确集内重新分配质量,揭示未采样正确质量可以收缩,即使总正确质量增长。受此分析的启发,我们提出了一个受Focal损失启发的难度感知优势缩放系数,该系数降低高成功提示的更新权重。轻量级修改可以直接集成到任何群组相对RLVR算法中,如GRPO、DAPO和CISPO。在Qwen2.5-7B上跨领域内和领域外基准测试,我们的方法将pass@256从64.1提高到70.3(GRPO)、69.3提高到72.5(DAPO)和73.2提高到76.8(CISPO),同时保留或提高pass@1,而不增加群组大小或计算成本。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06717v1</span>
                    <a href="https://arxiv.org/abs/2602.06717v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#18</span>
                <h2>AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Alisia Lupidi, Bhavul Gauri, Thomas Simon Foster 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">LLM智能体在推进科学研究方面具有重大前景。为了加速这一进展,我们引入了AIRS-Bench(AI研究科学基准),这是一套来自最先进机器学习论文的20个任务。这些任务跨越多个领域,包括语言建模、数学、生物信息学和时间序列预测。AIRS-Bench任务评估整个研究生命周期的智能体能力-包括想法生成、实验分析和迭代细化-而不提供基线代码。AIRS-Bench任务格式是通用的,可以轻松集成新任务并在不同智能体框架之间进行严格比较。我们使用配备序列和并行脚手架的前沿模型建立基线。我们的结果表明,智能体在四个任务中超过了人类SOTA,但在其他十六个任务中未能匹配。即使智能体超越人类基准,它们也没有达到基础任务的理论性能上限。这些发现表明AIRS-Bench远未饱和,并提供了大量改进空间。我们开源了AIRS-Bench任务定义和评估代码,以促进自主科学研究的进一步发展。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06855v2</span>
                    <a href="https://arxiv.org/abs/2602.06855v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#19</span>
                <h2>Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yalcin Tur, Jalal Naghiyev, Haoquan Fang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">当前的视觉-语言-动作(VLA)模型依赖于固定的计算深度,在简单调整和复杂的多步骤操纵上花费相同数量的计算。虽然思维链(CoT)提示实现了可变计算,但它线性扩展内存,不适合连续动作空间。我们引入递归深度VLA(RD-VLA),一种通过潜在迭代细化而非显式token生成实现计算自适应性的架构。RD-VLA采用递归、权重绑定的动作头,支持具有恒定内存占用的任意推理深度。该模型使用截断的时间反向传播(TBPTT)进行训练,以有效监督细化过程。在推理时,RD-VLA使用基于潜在收敛的自适应停止标准动态分配计算。在具有挑战性的操纵任务上的实验表明,递归深度至关重要:使用单次迭代推理完全失败(0%成功)的任务在四次迭代后超过90%成功,而更简单的任务迅速饱和。RD-VLA为机器人技术中的测试时计算提供了一条可扩展的路径,用潜在推理替代基于token的推理,实现恒定内存使用,并比先前基于推理的VLA模型快80倍。项目页面:https://rd-vla.github.io/</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07845v1</span>
                    <a href="https://arxiv.org/abs/2602.07845v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#20</span>
                <h2>UI-Venus-1.5 Technical Report</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors"> Veuns-Team,  :, Changlong Gao 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">GUI智能体已成为自动化数字环境交互的强大范式,但实现广泛通用性和持续强大的任务性能仍然具有挑战性。在本报告中,我们提出UI-Venus-1.5,一个统一的端到端GUI智能体,专为强大的真实世界应用而设计。所提出的模型系列包括两个密集变体(2B和8B)和一个混合专家变体(30B-A3B),以满足各种下游应用场景。与我们之前的版本相比,UI-Venus-1.5引入了三个关键技术进步:(1)利用30+数据集的100亿token的全面中期训练阶段,以建立基础GUI语义;(2)具有完整轨迹展开的在线强化学习,将训练目标与大规模环境中的长期动态导航对齐;(3)通过模型合并构建的单一统一GUI智能体,将特定领域模型(接地、网络和移动)合成为一个内聚检查点。广泛的评估表明,UI-Venus-1.5在ScreenSpot-Pro(69.6%)、VenusBench-GD(75.0%)和AndroidWorld(77.6%)等基准测试上建立了新的最先进性能,显著优于先前的强基线。此外,UI-Venus-1.5展示了跨各种中文移动应用程序的强大导航能力,有效执行真实世界场景中的用户指令。代码:https://github.com/inclusionAI/UI-Venus;模型:https://huggingface.co/collections/inclusionAI/ui-venus</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09082v1</span>
                    <a href="https://arxiv.org/abs/2602.09082v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#21</span>
                <h2>LLaDA2.1: Speeding Up Text Diffusion via Token Editing</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Tiwei Bie, Maosong Cao, Xiang Cao 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">虽然LLaDA2.0展示了100B级块扩散模型的扩展潜力及其固有的并行化,但解码速度和生成质量之间的微妙平衡一直是一个难以捉摸的前沿。今天,我们推出LLaDA2.1,一个旨在超越这种权衡的范式转变。通过将Token到Token(T2T)编辑无缝编织到传统的Mask到Token(M2T)方案中,我们引入了一个联合的、可配置的阈值解码方案。这种结构创新产生了两种不同的角色:速度模式(S模式),大胆降低M2T阈值以绕过传统约束,同时依赖T2T来细化输出;以及质量模式(Q模式),倾向于保守阈值以确保卓越的基准性能,管理效率降级。进一步推进这一演变,在广泛的上下文窗口支持下,我们实施了第一个专门为dLLM量身定制的大规模强化学习(RL)框架,由稳定梯度估计的专门技术锚定。这种对齐不仅提高了推理精度,而且提升了指令遵循保真度,弥合了扩散动态与复杂人类意图之间的鸿沟。我们通过发布LLaDA2.1-Mini(16B)和LLaDA2.1-Flash(100B)来完成这项工作。在33个严格的基准测试中,LLaDA2.1提供强大的任务性能和闪电般的解码速度。尽管其体积为100B,在编码任务上,它在HumanEval+上达到惊人的892 TPS,在BigCodeBench上达到801 TPS,在LiveCodeBench上达到663 TPS。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08676v2</span>
                    <a href="https://arxiv.org/abs/2602.08676v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#22</span>
                <h2>AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Georgii Aparin, Tasnima Sadekova, Alexey Rukhovich 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">稀疏自动编码器(SAE)是解释神经表示的强大工具,但它们在音频中的使用仍未得到充分探索。我们在Whisper和HuBERT的所有编码器层上训练SAE,对其稳定性、可解释性进行广泛评估,并展示其实际效用。超过50%的特征在随机种子之间保持一致,重建质量得以保留。SAE特征捕获一般声学和语义信息以及特定事件,包括环境噪声和副语言声音(例如笑声、耳语),并有效解开它们,只需要删除19-27%的特征即可擦除概念。特征操纵将Whisper的假语音检测减少70%,WER增加可忽略不计,展示了真实世界的适用性。最后,我们发现SAE特征与语音感知期间的人类EEG活动相关,表明与人类神经处理对齐。代码和检查点可在https://github.com/audiosae/audiosae_demo获取。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05027v2</span>
                    <a href="https://arxiv.org/abs/2602.05027v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#23</span>
                <h2>On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Shumin Wang, Yuexiang Xie, Wenhao Zhang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">熵是衡量大语言模型(LLM)生成的输出多样性的关键指标,为其探索能力提供了有价值的见解。虽然最近的研究越来越关注监控和调整熵,以更好地平衡强化微调(RFT)中的探索和利用,但对此过程中熵动态的原则性理解尚未得到彻底研究。在本文中,我们建立了一个理论框架来分析RFT过程中的熵动态,该框架始于一个判别表达式,该表达式量化单个logit更新下的熵变化。这个基础使得能够推导熵变化的一阶表达式,该表达式可以进一步扩展到群组相对策略优化(GRPO)的更新公式。从理论分析中得出的推论和见解启发了熵控制方法的设计,并为解释现有研究中各种基于熵的方法提供了统一的视角。我们提供经验证据来支持我们分析的主要结论,并展示了派生的熵判别器裁剪方法的有效性。这项研究为RFT训练动态提供了新颖的见解,为优化LLM微调过程中的探索-利用平衡提供了理论支持和实用策略。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.03392v1</span>
                    <a href="https://arxiv.org/abs/2602.03392v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#24</span>
                <h2>OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Fangzhi Xu, Hang Yan, Qiushi Sun 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">大语言模型(LLM)的快速发展催化了能够导航复杂环境的自主智能体的发展。然而,现有评估主要采用演绎范式,智能体根据明确提供的规则和静态目标执行任务,通常在有限的规划范围内。至关重要的是,这忽略了智能体从经验中自主发现潜在转换规律的归纳必要性,这是实现智能体远见和维持战略连贯性的基石。为了弥合这一差距,我们引入OdysseyArena,它将智能体评估重新集中在长期、主动和归纳交互上。我们形式化并实例化四个原语,将抽象转换动态转换为具体的交互环境。在此基础上,我们建立了OdysseyArena-Lite进行标准化基准测试,提供一组120个任务来衡量智能体的归纳效率和长期发现。进一步推进,我们引入OdysseyArena-Challenge来压力测试智能体在极端交互范围(例如>200步)下的稳定性。对15+领先LLM的广泛实验表明,即使是前沿模型也表现出归纳场景中的缺陷,识别出在复杂环境中追求自主发现的关键瓶颈。我们的代码和数据可在https://github.com/xufangzhi/Odyssey-Arena获取</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05843v1</span>
                    <a href="https://arxiv.org/abs/2602.05843v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#25</span>
                <h2>Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Baichuan-M3 Team,  :, Chengfeng Dou 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">我们引入Baichuan-M3,一个医疗增强的大语言模型,旨在将范式从被动问答转变为主动的临床级决策支持。针对现有系统在开放式咨询中的局限性,Baichuan-M3利用专门的训练流水线对医生的系统工作流程进行建模。关键能力包括:(i)主动信息获取以解决歧义;(ii)将分散证据统一为连贯诊断的长期推理;(iii)自适应幻觉抑制以确保事实可靠性。实证评估表明,Baichuan-M3在HealthBench、新引入的HealthBench-Hallu和ScanBench上取得了最先进的结果,在临床查询、咨询和安全方面显著优于GPT-5.2。这些模型可在https://huggingface.co/collections/baichuan-inc/baichuan-m3公开获取。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06570v1</span>
                    <a href="https://arxiv.org/abs/2602.06570v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#26</span>
                <h2>Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yunze Tong, Mushui Liu, Canyu Zhao 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">在流匹配模型上部署GRPO已被证明对文本到图像生成有效。然而,现有范式通常将基于结果的奖励传播到所有先前的去噪步骤,而不区分每个步骤的局部效果。此外,当前的群组排名主要在匹配的时间步长比较轨迹,并忽略轨迹内依赖关系,其中某些早期去噪动作可以通过延迟的隐式交互影响后续状态。我们提出TurningPoint-GRPO(TP-GRPO),一个GRPO框架,缓解步级奖励稀疏性并明确建模去噪轨迹内的长期效应。TP-GRPO做出两个关键创新:(i)它用步级增量奖励替换基于结果的奖励,提供密集的、步感知的学习信号,更好地隔离每个去噪动作的"纯"效果,(ii)它识别转折点-翻转局部奖励趋势并使后续奖励演变与整体轨迹趋势一致的步骤-并为这些动作分配聚合的长期奖励以捕获其延迟影响。转折点仅通过增量奖励的符号变化来检测,使TP-GRPO高效且无超参数。广泛的实验还表明,TP-GRPO更有效地利用奖励信号并持续改进生成。演示代码可在https://github.com/YunzeTong/TurningPoint-GRPO获取。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06422v1</span>
                    <a href="https://arxiv.org/abs/2602.06422v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#27</span>
                <h2>SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Peng Xia, Jianwen Chen, Hanyang Wang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">大语言模型(LLM)智能体在复杂任务中表现出惊人的结果,但它们经常孤立运行,无法从过去的经验中学习。现有的基于记忆的方法主要存储原始轨迹,这些轨迹通常是冗余和噪声密集的。这阻止智能体提取对泛化至关重要的高级、可重用的行为模式。在本文中,我们提出SkillRL,一个通过自动技能发现和递归演化弥合原始经验和策略改进之间差距的框架。我们的方法引入了一个基于经验的蒸馏机制来构建分层技能库SkillBank,一个用于通用和任务特定启发式的自适应检索策略,以及一个允许技能库在强化学习过程中与智能体策略共同演化的递归演化机制。这些创新显著减少了token占用,同时增强了推理效用。在ALFWorld、WebShop和七个搜索增强任务上的实验结果表明,SkillRL实现了最先进的性能,优于强基线超过15.3%,并随着任务复杂性的增加保持鲁棒性。代码可在此https://github.com/aiming-lab/SkillRL获取。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08234v1</span>
                    <a href="https://arxiv.org/abs/2602.08234v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#28</span>
                <h2>Towards Agentic Intelligence for Materials Science</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Huan Zhang, Yizhan Li, Wenhao Huang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">人工智能与材料科学的融合提供了一个变革性的机会,但实现发现的真正加速需要超越任务隔离的微调模型,转向在整个发现循环中规划、行动和学习的智能体系统。本综述提出了一个独特的以流水线为中心的视角,从语料库管理和预训练,通过领域适应和指令调优,到与仿真和实验平台接口的目标条件智能体。与先前的综述不同,我们将整个过程视为一个端到端系统,以针对有形的发现结果而非代理基准进行优化。这种观点使我们能够追踪上游设计选择-如数据管理和训练目标-如何通过有效的信用分配与下游实验成功对齐。为了弥合社区并建立共享的参考框架,我们首先提出一个集成视角,对齐AI和材料科学的术语、评估和工作流程阶段。然后,我们通过两个聚焦视角分析该领域:从AI角度,该综述详细介绍了LLM在模式识别、预测分析和用于文献挖掘、材料表征和性质预测的自然语言处理方面的优势;从材料科学角度,它突出了材料设计、过程优化以及通过与外部工具(例如DFT、机器人实验室)集成加速计算工作流程的应用。最后,我们将被动、反应方法与智能体设计进行对比,对当前贡献进行分类,同时激励追求具有自主性、记忆和工具使用的长期目标的系统。本综述绘制了一条通往自主、安全感知的LLM智能体的实用路线图,旨在发现新颖和有用的材料。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.00169v2</span>
                    <a href="https://arxiv.org/abs/2602.00169v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#29</span>
                <h2>InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Shiyang Feng, Runmin Ma, Xiangchao Yan 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">我们引入InternAgent-1.5,一个为跨计算和经验领域的端到端科学发现而设计的统一系统。该系统建立在由三个协调子系统组成的结构化架构上,用于生成、验证和演化。这些子系统由深度研究、解决方案优化和长期记忆的基础能力支持。该架构允许InternAgent-1.5在扩展的发现周期中持续运行,同时保持连贯和改进的行为。它还使系统能够在单个统一系统内协调计算建模和实验室实验。我们在GAIA、HLE、GPQA和FrontierScience等科学推理基准测试上评估InternAgent-1.5,该系统实现了领先性能,展示了强大的基础能力。除了这些基准测试,我们进一步评估了两类发现任务。在算法发现任务中,InternAgent-1.5自主设计核心机器学习问题的竞争方法。在经验发现任务中,它执行完整的计算或湿实验室实验,并在地球、生命、生物和物理领域产生科学发现。总体而言,这些结果表明InternAgent-1.5为自主科学发现提供了一个通用且可扩展的框架。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08990v1</span>
                    <a href="https://arxiv.org/abs/2602.08990v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#30</span>
                <h2>RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Hongzhi Zang, Shu'ang Yu, Hao Lin 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">直接在物理世界中进行在线策略学习是具身智能的一个有前景但具有挑战性的方向。与仿真不同,真实世界系统不能任意加速、廉价重置或大规模复制,这使得可扩展的数据收集、异构部署和长期有效训练变得困难。这些挑战表明,真实世界策略学习不仅是一个算法问题,而且从根本上是一个系统问题。我们提出USER,一个用于真实世界在线策略学习的统一且可扩展的系统。USER通过统一的硬件抽象层将物理机器人视为与GPU并列的一流硬件资源,实现异构机器人的自动发现、管理和调度。为了解决云边缘通信,USER引入了一个具有基于隧道的网络的自适应通信平面、用于流量本地化的分布式数据通道,以及流多处理器感知的权重同步以调节GPU端开销。在此基础设施之上,USER将学习组织为一个完全异步的框架,具有持久的、缓存感知的缓冲区,实现高效的长期实验,具有强大的崩溃恢复和历史数据的重用。此外,USER为奖励、算法和策略提供可扩展的抽象,在统一流水线内支持CNN/MLP、生成策略和大型视觉-语言-动作(VLA)模型的在线模仿或强化学习。在仿真和真实世界中的结果表明,USER实现了多机器人协调、异构机械手、具有大型模型的边缘云协作以及长期运行的异步训练,为真实世界在线策略学习提供了统一且可扩展的系统基础。</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07837v2</span>
                    <a href="https://arxiv.org/abs/2602.07837v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#31</span>
                <h2>P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yun Luo, Futing Wang, Qianjia Cheng 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09443v1</span>
                    <a href="https://arxiv.org/abs/2602.09443v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#32</span>
                <h2>Pisets: A Robust Speech Recognition System for Lectures and Interviews</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Ivan Bondarenko, Daniil Grebenkin, Oleg Sedukhin 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2601.18415v1</span>
                    <a href="https://arxiv.org/abs/2601.18415v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#33</span>
                <h2>Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yuhao Dong, Shulin Tian, Shuai Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08439v1</span>
                    <a href="https://arxiv.org/abs/2602.08439v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#34</span>
                <h2>MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Lianhai Ren, Yucheng Ding, Xiao Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.01734v1</span>
                    <a href="https://arxiv.org/abs/2602.01734v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#35</span>
                <h2>GEBench: Benchmarking Image Generation Models as GUI Environments</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Haodong Li, Jingwei Wu, Quan Sun 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09007v2</span>
                    <a href="https://arxiv.org/abs/2602.09007v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#36</span>
                <h2>Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Zhaoyang Wang, Canwen Xu, Boyi Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.10090v1</span>
                    <a href="https://arxiv.org/abs/2602.10090v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#37</span>
                <h2>Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Haozhen Zhang, Haodong Yue, Tao Feng 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06025v1</span>
                    <a href="https://arxiv.org/abs/2602.06025v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#38</span>
                <h2>Prism: Spectral-Aware Block-Sparse Attention</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Xinghao Wang, Pengyu Wang, Xiaoran Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08426v1</span>
                    <a href="https://arxiv.org/abs/2602.08426v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#39</span>
                <h2>DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Shenyuan Gao, William Liang, Kaiyuan Zheng 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06949v1</span>
                    <a href="https://arxiv.org/abs/2602.06949v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#40</span>
                <h2>Improving Data and Reward Design for Scientific Reasoning in Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Zijie Chen, Zhenghao Lin, Xiao Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr. SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08321v2</span>
                    <a href="https://arxiv.org/abs/2602.08321v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#41</span>
                <h2>QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Jun Han, Shuo Zhang, Wei Li 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07085v1</span>
                    <a href="https://arxiv.org/abs/2602.07085v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#42</span>
                <h2>Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Xiaomin Yu, Yi Xin, Wenjie Zhang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07026v1</span>
                    <a href="https://arxiv.org/abs/2602.07026v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#43</span>
                <h2>MOVA: Towards Scalable and Synchronized Video-Audio Generation</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">SII-OpenMOSS Team,  :, Donghua Yu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08794v2</span>
                    <a href="https://arxiv.org/abs/2602.08794v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#44</span>
                <h2>OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Shaobo Wang, Xuan Ouyang, Tianyi Xu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05400v2</span>
                    <a href="https://arxiv.org/abs/2602.05400v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#45</span>
                <h2>Weak-Driven Learning: How Weak Agents make Strong Agents Stronger</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Zehao Chen, Gongxun Li, Tianxiang Ai 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08222v1</span>
                    <a href="https://arxiv.org/abs/2602.08222v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#46</span>
                <h2>Code2World: A GUI World Model via Renderable Code Generation</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yuhao Zheng, Li'an Zhong, Yi Wang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09856v1</span>
                    <a href="https://arxiv.org/abs/2602.09856v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#47</span>
                <h2>F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Daniil Plyusov, Alexey Gorbatovski, Boris Shaposhnikov 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 $\rightarrow$ 70.3 (GRPO), 69.3 $\rightarrow$ 72.5 (DAPO), and 73.2 $\rightarrow$ 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06717v1</span>
                    <a href="https://arxiv.org/abs/2602.06717v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#48</span>
                <h2>AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Alisia Lupidi, Bhavul Gauri, Thomas Simon Foster 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06855v2</span>
                    <a href="https://arxiv.org/abs/2602.06855v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#49</span>
                <h2>Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yalcin Tur, Jalal Naghiyev, Haoquan Fang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07845v1</span>
                    <a href="https://arxiv.org/abs/2602.07845v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#50</span>
                <h2>UI-Venus-1.5 Technical Report</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors"> Veuns-Team,  :, Changlong Gao 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09082v1</span>
                    <a href="https://arxiv.org/abs/2602.09082v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#51</span>
                <h2>LLaDA2.1: Speeding Up Text Diffusion via Token Editing</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Tiwei Bie, Maosong Cao, Xiang Cao 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08676v2</span>
                    <a href="https://arxiv.org/abs/2602.08676v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#52</span>
                <h2>AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Georgii Aparin, Tasnima Sadekova, Alexey Rukhovich 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05027v2</span>
                    <a href="https://arxiv.org/abs/2602.05027v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#53</span>
                <h2>On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Shumin Wang, Yuexiang Xie, Wenhao Zhang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.03392v1</span>
                    <a href="https://arxiv.org/abs/2602.03392v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#54</span>
                <h2>OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Fangzhi Xu, Hang Yan, Qiushi Sun 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.05843v1</span>
                    <a href="https://arxiv.org/abs/2602.05843v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#55</span>
                <h2>Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Baichuan-M3 Team,  :, Chengfeng Dou 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06570v1</span>
                    <a href="https://arxiv.org/abs/2602.06570v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#56</span>
                <h2>Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yunze Tong, Mushui Liu, Canyu Zhao 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06422v1</span>
                    <a href="https://arxiv.org/abs/2602.06422v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#57</span>
                <h2>SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Peng Xia, Jianwen Chen, Hanyang Wang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08234v1</span>
                    <a href="https://arxiv.org/abs/2602.08234v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#58</span>
                <h2>Towards Agentic Intelligence for Materials Science</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Huan Zhang, Yizhan Li, Wenhao Huang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.   To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.00169v2</span>
                    <a href="https://arxiv.org/abs/2602.00169v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#59</span>
                <h2>InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Shiyang Feng, Runmin Ma, Xiangchao Yan 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08990v1</span>
                    <a href="https://arxiv.org/abs/2602.08990v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#60</span>
                <h2>RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Hongzhi Zang, Shu'ang Yu, Hao Lin 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07837v2</span>
                    <a href="https://arxiv.org/abs/2602.07837v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#61</span>
                <h2>P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yun Luo, Futing Wang, Qianjia Cheng 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09443v1</span>
                    <a href="https://arxiv.org/abs/2602.09443v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#62</span>
                <h2>Pisets: A Robust Speech Recognition System for Lectures and Interviews</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Ivan Bondarenko, Daniil Grebenkin, Oleg Sedukhin 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2601.18415v1</span>
                    <a href="https://arxiv.org/abs/2601.18415v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#63</span>
                <h2>Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yuhao Dong, Shulin Tian, Shuai Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08439v1</span>
                    <a href="https://arxiv.org/abs/2602.08439v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#64</span>
                <h2>MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Lianhai Ren, Yucheng Ding, Xiao Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.01734v1</span>
                    <a href="https://arxiv.org/abs/2602.01734v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#65</span>
                <h2>GEBench: Benchmarking Image Generation Models as GUI Environments</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Haodong Li, Jingwei Wu, Quan Sun 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09007v2</span>
                    <a href="https://arxiv.org/abs/2602.09007v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#66</span>
                <h2>Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Zhaoyang Wang, Canwen Xu, Boyi Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.10090v1</span>
                    <a href="https://arxiv.org/abs/2602.10090v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#67</span>
                <h2>Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Haozhen Zhang, Haodong Yue, Tao Feng 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06025v1</span>
                    <a href="https://arxiv.org/abs/2602.06025v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#68</span>
                <h2>Prism: Spectral-Aware Block-Sparse Attention</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Xinghao Wang, Pengyu Wang, Xiaoran Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08426v1</span>
                    <a href="https://arxiv.org/abs/2602.08426v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#69</span>
                <h2>DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Shenyuan Gao, William Liang, Kaiyuan Zheng 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06949v1</span>
                    <a href="https://arxiv.org/abs/2602.06949v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#70</span>
                <h2>Improving Data and Reward Design for Scientific Reasoning in Large Language Models</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Zijie Chen, Zhenghao Lin, Xiao Liu 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr. SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08321v2</span>
                    <a href="https://arxiv.org/abs/2602.08321v2" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#71</span>
                <h2>GISA: A Benchmark for General Information-Seeking Assistant</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yutao Zhu, Xingshuo Zhang, Maosen Zhang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.08543v1</span>
                    <a href="https://arxiv.org/abs/2602.08543v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#72</span>
                <h2>LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Weihao Zeng, Yuzhen Huang, Junxian He</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07962v1</span>
                    <a href="https://arxiv.org/abs/2602.07962v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#73</span>
                <h2>Chain of Mindset: Reasoning with Adaptive Cognitive Modes</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Tianyi Jiang, Arctanx An, Hengyi Feng 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.10063v1</span>
                    <a href="https://arxiv.org/abs/2602.10063v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#74</span>
                <h2>Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Guijin Son, Donghun Yang, Hitesh Laxmichand Patel 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06291v1</span>
                    <a href="https://arxiv.org/abs/2602.06291v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#75</span>
                <h2>Self-Improving World Modelling with Latent Actions</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yifu Qiu, Zheng Zhao, Waylon Li 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Internal modelling of the world -- predicting transitions between previous states $X$ and next states $Y$ under actions $Z$ -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) $P_θ(Y|X,Z)$ and an Inverse Dynamics Modelling (IDM) $Q_φ(Z|X,Y)$. SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06130v1</span>
                    <a href="https://arxiv.org/abs/2602.06130v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#76</span>
                <h2>AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Yishan Li, Wentong Chen, Yukun Yan 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.06540v1</span>
                    <a href="https://arxiv.org/abs/2602.06540v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#77</span>
                <h2>DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Jiahao Zhao, Shaoxuan Xu, Zhongxiang Sun 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07035v1</span>
                    <a href="https://arxiv.org/abs/2602.07035v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#78</span>
                <h2>Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Pingyue Zhang, Zihan Huang, Yue Wang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07055v1</span>
                    <a href="https://arxiv.org/abs/2602.07055v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#79</span>
                <h2>TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Kaijie Zhu, Yuzhou Nie, Yijiang Li 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.07274v1</span>
                    <a href="https://arxiv.org/abs/2602.07274v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

            <div class="paper">
                <span class="paper-number">#80</span>
                <h2>WorldCompass: Reinforcement Learning for Long-Horizon World Models</h2>
                
                <div class="paper-meta">
                    <strong>作者：</strong>
                    <span class="authors">Zehan Wang, Tengfei Wang, Haiyu Zhang 等</span>
                </div>
                
                <div class="summary">
                    <div class="summary-label">📄 摘要</div>
                    <div class="summary-text">This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.</div>
                </div>
                
                <div class="arxiv-info">
                    <span class="arxiv-id">arXiv: 2602.09022v1</span>
                    <a href="https://arxiv.org/abs/2602.09022v1" target="_blank" class="arxiv-link">
                        📖 查看论文原文
                    </a>
                </div>
            </div>

        </div>
        
        <div class="footer">
            <p>数据来源：<a href="https://huggingface.co/papers/week/2026-W07" target="_blank">Hugging Face Papers (2026-W07)</a></p>
            <p style="margin-top: 10px; opacity: 0.8;">生成时间：2026-02-11 16:38 (GMT+8) | 共 80 篇论文</p>
        </div>
    </div>
</body>
</html>
