#!/usr/bin/env python3
"""
çˆ¬å–HuggingFace Papersçš„å¤šå‘¨æ•°æ®ï¼Œè‡ªåŠ¨ç¿»è¯‘æ‘˜è¦ä¸ºä¸­æ–‡
"""

import json
import urllib.request
import urllib.parse
import time
import re
from datetime import datetime

def fetch_huggingface_papers(week):
    """
    ä»HuggingFace APIè·å–æŒ‡å®šå‘¨çš„è®ºæ–‡
    weekæ ¼å¼: YYYY-WWW (ä¾‹å¦‚: 2026-W06)
    """
    api_url = f'https://huggingface.co/api/daily_papers?week={week}'
    
    print(f"\nğŸ” æ­£åœ¨çˆ¬å– {week} çš„è®ºæ–‡...")
    
    try:
        with urllib.request.urlopen(api_url, timeout=30) as response:
            data = json.loads(response.read().decode('utf-8'))
        
        papers = []
        for item in data:
            paper = item.get('paper', {})
            
            # æå–è®ºæ–‡ID (å»é™¤vç‰ˆæœ¬å·)
            arxiv_id = paper.get('id', '')
            clean_id = re.sub(r'v\d+$', '', arxiv_id)
            
            # æ„å»ºè®ºæ–‡æ•°æ®
            paper_data = {
                'title': paper.get('title', ''),
                'authors': ', '.join([author.get('name', '') for author in paper.get('authors', [])]),
                'summary': paper.get('summary', ''),  # åŸå§‹è‹±æ–‡æ‘˜è¦
                'abstract': '',  # å°†ç”¨äºä¸­æ–‡ç¿»è¯‘
                'arxiv_id': arxiv_id,
                'url': f'https://arxiv.org/abs/{clean_id}',
                'published': paper.get('publishedAt', ''),
                'upvotes': item.get('upvotes', 0)
            }
            
            papers.append(paper_data)
        
        print(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
        return papers
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        return []

def translate_abstract_to_chinese(english_text, title):
    """
    å°†è‹±æ–‡æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡
    è¿™é‡Œä½¿ç”¨ç®€å•çš„æ ‡è®°ï¼Œå®é™…ç¿»è¯‘éœ€è¦åœ¨åç»­æ­¥éª¤å®Œæˆ
    """
    # æ ‡è®°ä¸ºå¾…ç¿»è¯‘
    return {
        'original': english_text,
        'title': title,
        'status': 'pending'
    }

def create_archive(week, papers):
    """åˆ›å»ºå‘¨å­˜æ¡£æ–‡ä»¶"""
    # è§£æå‘¨æ¬¡ä¿¡æ¯
    year, week_num = week.split('-W')
    
    # è·å–è¯¥å‘¨çš„æ—¥æœŸï¼ˆå‘¨ä¸€ï¼‰
    from datetime import datetime, timedelta
    jan_4 = datetime(int(year), 1, 4)
    week_start = jan_4 + timedelta(weeks=int(week_num)-1, days=-jan_4.weekday())
    date_str = week_start.strftime('%Y-%m-%d')
    
    archive = {
        'week': week,
        'date': date_str,
        'count': len(papers),
        'papers': papers
    }
    
    return archive

def main():
    print("=" * 70)
    print("ğŸš€ HuggingFace Papers å¤šå‘¨çˆ¬è™«")
    print("=" * 70)
    
    # è¦çˆ¬å–çš„å‘¨æ¬¡
    weeks = ['2026-W06', '2026-W07']
    
    all_archives = []
    total_papers = 0
    
    for week in weeks:
        # è·å–è®ºæ–‡
        papers = fetch_huggingface_papers(week)
        
        if papers:
            # åˆ›å»ºå­˜æ¡£
            archive = create_archive(week, papers)
            all_archives.append(archive)
            total_papers += len(papers)
            
            # ä¿å­˜å•å‘¨å­˜æ¡£
            archive_path = f'/data/workspace/papers-weekly-site/archives/{week}.json'
            with open(archive_path, 'w', encoding='utf-8') as f:
                json.dump(archive, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ å·²ä¿å­˜: {archive_path}")
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(2)
    
    # åˆ›å»ºå­˜æ¡£ç´¢å¼•
    index = {
        'archives': [
            {
                'week': arch['week'],
                'date': arch['date'],
                'count': arch['count']
            }
            for arch in sorted(all_archives, key=lambda x: x['week'], reverse=True)
        ]
    }
    
    index_path = '/data/workspace/papers-weekly-site/archives/index.json'
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 70)
    print("âœ… çˆ¬å–å®Œæˆï¼")
    print("=" * 70)
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"  çˆ¬å–å‘¨æ•°: {len(weeks)}")
    print(f"  è®ºæ–‡æ€»æ•°: {total_papers}")
    print(f"  å­˜æ¡£æ–‡ä»¶: {len(all_archives)}")
    print()
    
    # ä¿å­˜å¾…ç¿»è¯‘åˆ—è¡¨
    pending_translations = []
    for archive in all_archives:
        for paper in archive['papers']:
            if paper.get('summary'):
                pending_translations.append({
                    'week': archive['week'],
                    'title': paper['title'],
                    'english': paper['summary']
                })
    
    translation_file = '/data/workspace/papers-weekly-site/pending_translations.json'
    with open(translation_file, 'w', encoding='utf-8') as f:
        json.dump(pending_translations, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ å¾…ç¿»è¯‘æ‘˜è¦: {len(pending_translations)} ç¯‡")
    print(f"ğŸ’¾ ç¿»è¯‘åˆ—è¡¨å·²ä¿å­˜: {translation_file}")
    print()
    print("â­ï¸  ä¸‹ä¸€æ­¥: è¿è¡Œç¿»è¯‘è„šæœ¬å°†æ‰€æœ‰æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡")

if __name__ == '__main__':
    main()
